{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T10:59:12.654168Z",
          "iopub.status.busy": "2024-06-25T10:59:12.653797Z",
          "iopub.status.idle": "2024-06-25T11:00:25.382619Z",
          "shell.execute_reply": "2024-06-25T11:00:25.381503Z",
          "shell.execute_reply.started": "2024-06-25T10:59:12.654134Z"
        },
        "id": "hgVYWoQIvyjG",
        "outputId": "af402aba-4f9b-4d4b-9c89-356e2cc706f0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
            "ERROR: No matching distribution found for faiss-gpu\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.2 -> 24.1.1\n",
            "[notice] To update, run: C:\\Users\\ali18\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.1.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-core<0.3,>=0.2.15 (from langgraph)\n",
            "  Using cached langchain_core-0.2.17-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.2.15->langgraph) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.2.15->langgraph)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.2.15->langgraph)\n",
            "  Downloading langsmith-0.1.85-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.2.15->langgraph) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.2.15->langgraph) (2.7.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.2.15->langgraph) (8.2.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.15->langgraph)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.15->langgraph) (3.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.15->langgraph) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.15->langgraph) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.15->langgraph) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.15->langgraph) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.15->langgraph) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.15->langgraph) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.15->langgraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ali18\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.15->langgraph) (2023.7.22)\n",
            "Downloading langgraph-0.1.8-py3-none-any.whl (91 kB)\n",
            "   ---------------------------------------- 0.0/91.4 kB ? eta -:--:--\n",
            "   ------------- -------------------------- 30.7/91.4 kB 660.6 kB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 41.0/91.4 kB 393.8 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 91.4/91.4 kB 744.3 kB/s eta 0:00:00\n",
            "Downloading langchain_core-0.2.17-py3-none-any.whl (366 kB)\n",
            "   ---------------------------------------- 0.0/366.1 kB ? eta -:--:--\n",
            "   ---------- ----------------------------- 92.2/366.1 kB 2.6 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 122.9/366.1 kB 1.8 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 235.5/366.1 kB 1.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 366.1/366.1 kB 2.3 MB/s eta 0:00:00\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.85-py3-none-any.whl (127 kB)\n",
            "   ---------------------------------------- 0.0/127.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 127.9/127.9 kB 3.7 MB/s eta 0:00:00\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: jsonpointer, jsonpatch, langsmith, langchain-core, langgraph\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.17 langgraph-0.1.8 langsmith-0.1.85\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.3.2 -> 24.1.1\n",
            "[notice] To update, run: C:\\Users\\ali18\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain\\\n",
        "    langchain-community\\\n",
        "    langchain-together\\\n",
        "    langchain-core\\\n",
        "    faiss-cpu\\\n",
        "    faiss-gpu\\\n",
        "    langgraph\\\n",
        "    sentence-transformers\\\n",
        "    gradio\\\n",
        "    requests\\\n",
        "    beautifulsoup4\\\n",
        "    unstructured[pdf]\\\n",
        "    langchain-huggingface\\\n",
        "    rank_bm25\\\n",
        "    tavily-python\n",
        "\n",
        "!pip install langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:00:25.385407Z",
          "iopub.status.busy": "2024-06-25T11:00:25.384990Z",
          "iopub.status.idle": "2024-06-25T11:00:26.350804Z",
          "shell.execute_reply": "2024-06-25T11:00:26.350038Z",
          "shell.execute_reply.started": "2024-06-25T11:00:25.385348Z"
        },
        "id": "DpPD-c_AvyjI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import os.path\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from textwrap import dedent\n",
        "\n",
        "from langchain_community.document_loaders import OnlinePDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_together import ChatTogether\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages.base import BaseMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.utilities.tavily_search import TavilySearchAPIWrapper\n",
        "from langchain.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.runnables import chain\n",
        "from langchain_core.documents.base import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from IPython.core.display import Markdown\n",
        "\n",
        "from typing import Literal, TypedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3NA2orYb_ku"
      },
      "outputs": [],
      "source": [
        "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"9f24f1264668e13d395b5f214fce62f405544086d879b3fa954b5578de15b72d\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbmiyPEUvyjJ"
      },
      "source": [
        "## Part 1 - Receiving and Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:00:26.352934Z",
          "iopub.status.busy": "2024-06-25T11:00:26.352227Z",
          "iopub.status.idle": "2024-06-25T11:00:26.462484Z",
          "shell.execute_reply": "2024-06-25T11:00:26.461574Z",
          "shell.execute_reply.started": "2024-06-25T11:00:26.352900Z"
        },
        "id": "rNXv-RqcvyjJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def scrap_and_get_links(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Failed scrap\")\n",
        "        return None\n",
        "    html_content = response.content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    all_hrefs = soup.find_all('a', href=True)\n",
        "    pdf_hrefs = [link['href'] for link in all_hrefs if re.search(r'^\\d+\\.pdf$', link['href'])]\n",
        "    links = [urljoin(url, link) for link in pdf_hrefs]\n",
        "    print(\"Success scrap\")\n",
        "    return links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_OYFF2aFudh",
        "outputId": "fe7769ab-e14b-4e8c-f38b-587011493b13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success scrap\n"
          ]
        }
      ],
      "source": [
        "url = 'https://stanford.edu/~jurafsky/slp3/'\n",
        "links = scrap_and_get_links(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:00:26.485740Z",
          "iopub.status.busy": "2024-06-25T11:00:26.485464Z",
          "iopub.status.idle": "2024-06-25T11:02:35.813821Z",
          "shell.execute_reply": "2024-06-25T11:02:35.813041Z",
          "shell.execute_reply.started": "2024-06-25T11:00:26.485716Z"
        },
        "id": "V-wCLCVgvyjK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for link in links:\n",
        "    online_pdf_loader = OnlinePDFLoader(link)\n",
        "    documents.extend(online_pdf_loader.load())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:02:35.815278Z",
          "iopub.status.busy": "2024-06-25T11:02:35.814856Z",
          "iopub.status.idle": "2024-06-25T11:02:36.071857Z",
          "shell.execute_reply": "2024-06-25T11:02:36.070933Z",
          "shell.execute_reply.started": "2024-06-25T11:02:35.815252Z"
        },
        "id": "WgtYEh0ovyjL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "recursive_character_text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=64,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "chunks = recursive_character_text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyZDmHBvvyjM"
      },
      "source": [
        "## Part 2 - Vector Embedding and Database Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:03:23.821609Z",
          "iopub.status.busy": "2024-06-25T11:03:23.821024Z",
          "iopub.status.idle": "2024-06-25T11:03:25.113796Z",
          "shell.execute_reply": "2024-06-25T11:03:25.112926Z",
          "shell.execute_reply.started": "2024-06-25T11:03:23.821577Z"
        },
        "id": "KAmQ3xcIvyjM",
        "outputId": "6b39d9c2-b8cf-4256-cd85-dca3ea99492a",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "store = LocalFileStore('./cache/')\n",
        "general_embedding = HuggingFaceEmbeddings()\n",
        "embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
        "    general_embedding,\n",
        "    store\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KZDwWuoZLCXg"
      },
      "outputs": [],
      "source": [
        "def load_faiss_vector_store(embeddings_db_name):\n",
        "    if not os.path.isfile(embeddings_db_name):\n",
        "        faiss_vector_store = FAISS.from_documents(chunks, embeddings)\n",
        "        faiss_vector_store.save_local(embeddings_db_name)\n",
        "    else:\n",
        "        faiss_vector_store = FAISS.load_local(embeddings_db_name)\n",
        "\n",
        "    return faiss_vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:03:35.691405Z",
          "iopub.status.busy": "2024-06-25T11:03:35.690532Z",
          "iopub.status.idle": "2024-06-25T11:04:02.027973Z",
          "shell.execute_reply": "2024-06-25T11:04:02.027084Z",
          "shell.execute_reply.started": "2024-06-25T11:03:35.691348Z"
        },
        "id": "L8ZY043PvyjN",
        "outputId": "f39229a6-a513-4ba7-9ab5-5557574f0c8e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2122\n"
          ]
        }
      ],
      "source": [
        "faiss_vector_store = load_faiss_vector_store(\n",
        "    embeddings_db_name=\"faiss_index\"\n",
        ")\n",
        "\n",
        "print(faiss_vector_store.index.ntotal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qsBhswgvyjN"
      },
      "source": [
        "## Part 3 - Implementing Semantic and Lexical Retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:09:08.802317Z",
          "iopub.status.busy": "2024-06-25T11:09:08.801794Z",
          "iopub.status.idle": "2024-06-25T11:09:09.063316Z",
          "shell.execute_reply": "2024-06-25T11:09:09.062523Z",
          "shell.execute_reply.started": "2024-06-25T11:09:08.802285Z"
        },
        "id": "Ji0OhGvCvyjN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "bm25_term_saturation = 4\n",
        "bm25_retriever = BM25Retriever.from_texts(\n",
        "    [chunk.page_content for chunk in chunks]\n",
        ")\n",
        "\n",
        "bm25_retriever.k = bm25_term_saturation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:09:14.696517Z",
          "iopub.status.busy": "2024-06-25T11:09:14.695896Z",
          "iopub.status.idle": "2024-06-25T11:09:14.700762Z",
          "shell.execute_reply": "2024-06-25T11:09:14.699767Z",
          "shell.execute_reply.started": "2024-06-25T11:09:14.696484Z"
        },
        "id": "OoM6pZUXvyjO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "faiss_term_saturation = 4\n",
        "faiss_retriever = faiss_vector_store.as_retriever(\n",
        "    search_kwargs={'k': faiss_term_saturation}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-25T11:09:52.774189Z",
          "iopub.status.busy": "2024-06-25T11:09:52.773314Z",
          "iopub.status.idle": "2024-06-25T11:09:52.798243Z",
          "shell.execute_reply": "2024-06-25T11:09:52.797329Z",
          "shell.execute_reply.started": "2024-06-25T11:09:52.774154Z"
        },
        "id": "RvtR8dKCvyjO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "related_query = \"Named Entity Recognizers\"\n",
        "unrelated_query = \"Cloud Computing\"\n",
        "super_unrelated_query = \"Champion of Asian cup\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdW7TsmyOEsF",
        "outputId": "d0cebfb1-1e92-4042-8388-cadb2bebc62d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'source': '/tmp/tmpz3h9atcu/tmp.pdf'}, page_content='named entity\\n\\nnamed entity recognition NER\\n\\nA named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recog- nition (NER) is to ﬁnd spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren’t entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here’s an example of the output of an NER tagger:'), Document(metadata={'source': '/tmp/tmpz3h9atcu/tmp.pdf'}, page_content='8.6 Evaluation of Named Entity Recognition\\n\\nPart-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F-measure is the harmonic mean of the two.\\n\\nTo know if the difference between the F1 scores of two NER systems is a signif- icant difference, we use the paired bootstrap test, or the similar randomization test (Section ??).\\n\\nFor named entity tagging, the entity rather than the word is the unit of response. Thus in the example in Fig. 8.16, the two entities Jane Villanueva and United Air- lines Holding and the non-entity discussed would each count as a single response.'), Document(metadata={'source': '/tmp/tmpz3h9atcu/tmp.pdf'}, page_content='8.5.2 Features for CRF Named Entity Recognizers\\n\\nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15.\\n\\nidentity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words presence of wi in a gazetteer wi contains a particular preﬁx (from all preﬁxes of length ≤ 4) wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4) word shape of wi, word shape of neighboring words short word shape of wi, short word shape of neighboring words gazetteer features Figure 8.15 Typical features for a feature-based NER system.'), Document(metadata={'source': '/tmp/tmpdc4enbfj/tmp.pdf'}, page_content='[CLS]Janetwillbackthebill\\n\\nEmbeddingLayer\\n\\nNNPMDVBDTNN\\n\\nBidirectional Transformer Encoder\\n\\nFigure 11.11 Sequence labeling for part-of-speech tagging with a bidirectional transformer encoder. The output vector for each input token is passed to a simple k-way classiﬁer.\\n\\nA complication with this approach arises from the use of subword tokenization such as WordPiece, SentencePiece Unigram LM or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:\\n\\n(11.14)\\n\\n(11.15)\\n\\n11.5\\n\\nADVANCED: SPAN-BASED MASKING 17\\n\\n[LOC Mt. Sanitas ] is in [LOC Sunshine Canyon] .\\n\\nwould have the following set of per-word BIO tags.\\n\\n(11.16) Mt.\\n\\nB-LOC\\n\\nSanitas I-LOC\\n\\nis O\\n\\nin O\\n\\nSunshine B-LOC\\n\\nCanyon I-LOC\\n\\n. O')]\n",
            "[Document(metadata={'source': '/tmp/tmp8h47rjv0/tmp.pdf'}, page_content='18 CHAPTER 15\\n\\nCHATBOTS & DIALOGUE SYSTEMS\\n\\nthink is a program but is in fact a human “wizard” disguised by a software interface (Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name comes from the chil- dren’s book The Wizard of Oz (Baum, 1900), in which the wizard turned out to be a simu- lation controlled by a man behind a curtain or screen. A wizard system can be used to test out an architecture before implementation; only the interface software and databases need to be in place. The wizard gets input from the user, uses a database interface to run queries based on the user utterance, and then outputs sentences, ei- ther by typing them or speaking them.\\n\\nWizard-of-Oz systems are not a perfect simulation, since the wizard doesn’t exactly simulate the errors or limitations of a real sys- tem; but wizard studies can still provide a useful ﬁrst idea of the domain issues.\\n\\nbarged in\\n\\nvalue sensitive design'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='Basing our answers on retrieved documents can solve the above-mentioned prob- lems with using simple prompting to answer questions. First, we can ensure that the answer is grounded in facts from some curated dataset. And we can give the answer accompanied by the context of the passage or document the answer came from. This information can help users have conﬁdence in the accuracy of the answer (or help them spot when it is wrong!). And we can use our retrieval techniques on any pro- prietary data we want, such as legal or medical data for those applications.'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='20 CHAPTER 14\\n\\nQUESTION ANSWERING AND INFORMATION RETRIEVAL\\n\\nThe match between a query and a document can be done by ﬁrst representing each of them with a sparse vector that represents the frequencies of words, weighted by tf-idf or BM25. Then the similarity can be measured by cosine. • Documents or queries can instead be represented by dense vectors, by encod- ing the question and document with an encoder-only model like BERT, and in that case computing similarity in embedding space.\\n\\nThe inverted index is an storage mechanism that makes it very efﬁcient to ﬁnd documents that have a particular word.\\n\\nRanked retrieval is generally evaluated by mean average precision or inter- polated precision.\\n\\nQuestion answering systems generally use the retriever/reader architecture. In the retriever stage, an IR system is given a query and returns a set of documents.'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='Speech and Language Processing. Daniel Jurafsky & James H. Martin. rights reserved.\\n\\nDraft of February 3, 2024.\\n\\nCopyright © 2023.\\n\\nCHAPTER\\n\\n14 Question Answering and In-\\n\\nformation Retrieval\\n\\nThe quest for knowledge is deeply human, and so it is not surprising that practically as soon as there were computers we were asking them questions. By the early 1960s, systems used the two major paradigms of question answering—retrieval-based and knowledge-based—to answer questions about baseball statistics or scientiﬁc facts. Even imaginary computers got into the act. Deep Thought, the computer that Dou- glas Adams invented in The Hitchhiker’s Guide to the Galaxy, managed to answer “the Ultimate Question Of Life, The Universe, and Everything”.1 In 2011, IBM’s Watson question-answering system won the TV game-show Jeopardy!, surpassing humans at answering questions like:\\n\\nWILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA”INSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL\\n\\n2')]\n",
            "[Document(metadata={'source': '/tmp/tmp16hw7jef/tmp.pdf'}, page_content='Sometime between the 7th and 4th centuries BCE, the Indian grammarian P¯an. ini1 wrote a famous treatise on Sanskrit grammar, the As.t.¯adhy¯ay¯ı (‘8 books’), a treatise that has been called “one of the greatest monuments of hu- man intelligence” (Bloomﬁeld, 1933, 11). The work de- scribes the linguistics of the Sanskrit language in the form of 3959 sutras, each very efﬁciently (since it had to be memorized!) expressing part of a formal rule system that brilliantly preﬁgured modern mechanisms of formal lan- guage theory (Penn and Kiparsky, 2012). One set of rules describes the k¯arakas, semantic relationships between a verb and noun arguments, roles like agent, instrument, or destination. P¯an. ini’s work was the earliest we know of that modeled the linguistic realization of events and their participants. This task of understanding how participants relate to events—being able to answer the question “Who did what to whom” (and perhaps also “when and where”)—is a central question of natural language'), Document(metadata={'source': '/tmp/tmpkxwhtz_7/tmp.pdf'}, page_content='(13.1) English: He wrote a letter to a friend\\n\\nJapanese:\\n\\ntomodachi friend\\n\\nni to\\n\\ntegami-o letter\\n\\nkaita wrote\\n\\nNote that the elements of the sentences are in very different places in the different languages. In English, the verb is in the middle of the sentence, while in Japanese, the verb kaita comes at the end. The Japanese sentence doesn’t require the pronoun he, while English does.\\n\\nSuch differences between languages can be quite complex. In the following ac- tual sentence from the United Nations, notice the many changes between the Chinese sentence (we’ve given in red a word-by-word gloss of the Chinese characters) and its English equivalent produced by human translators. (13.2) 大会/General Assembly 在/on 1982年/1982 12月/December 10日/10 通过 了/adopted 第37号/37th 决议/resolution ，核准了/approved 第二 次/second 探索/exploration 及/and 和平peaceful colorblue利用/using 外 } 层空间/outer space 会议/conference 的/of 各项/various 建议/suggestions 。'), Document(metadata={'source': '/tmp/tmp16hw7jef/tmp.pdf'}, page_content='In this chapter we introduce a level of representation that captures the common- ality between these sentences: there was a purchase event, the participants were XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic representations , semantic roles, express the role that arguments of a predicate take in the event, codiﬁed in databases like PropBank and FrameNet. We’ll introduce semantic role labeling, the task of assigning roles to spans in sentences, and selec- tional restrictions, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible.\\n\\n1 Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based on the Sanskrit grammar of Panini. Image from the Wellcome Collection.\\n\\n2 CHAPTER 20\\n\\nSEMANTIC ROLE LABELING\\n\\n20.1 Semantic Roles'), Document(metadata={'source': '/tmp/tmpnpvt6que/tmp.pdf'}, page_content='parsing. ACL.\\n\\nNaur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz, J. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson, B. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and M. Woodger. 1960. Report on the algorithmic language ALGOL 60. CACM, 3(5):299–314. Revised in CACM 6:1, 1-17, 1963.\\n\\nNorvig, P. 1991. Techniques for automatic memoization with applications to context-free parsing. Computational Lin- guistics, 17(1):91–98.\\n\\nPercival, W. K. 1976. On the historical source of immediate constituent analysis. In J. D. McCawley, editor, Syntax and Semantics Volume 7, Notes from the Linguistic Un- derground, pages 229–242. Academic Press.\\n\\nPollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\\n\\nture Grammar. University of Chicago Press.\\n\\nSalomaa, A. 1969. Probabilistic and weighted grammars.\\n\\nInformation and Control, 15:529–544.\\n\\nSekine, S. and M. Collins. 1997. The evalb software. http:\\n\\n//cs.nyu.edu/cs/projects/proteus/evalb.\\n\\n25\\n\\n26 Chapter 17\\n\\nContext-Free Grammars and Constituency Parsing')]\n"
          ]
        }
      ],
      "source": [
        "print(faiss_retriever.invoke(related_query))\n",
        "print(faiss_retriever.invoke(unrelated_query))\n",
        "print(faiss_retriever.invoke(super_unrelated_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-06-25T11:10:03.255175Z",
          "iopub.status.busy": "2024-06-25T11:10:03.254440Z",
          "iopub.status.idle": "2024-06-25T11:10:03.267224Z",
          "shell.execute_reply": "2024-06-25T11:10:03.266360Z",
          "shell.execute_reply.started": "2024-06-25T11:10:03.255143Z"
        },
        "id": "_kY02eA0vyjP",
        "outputId": "5ba6b5f4-a8b1-4a60-f23f-aba50a53edea",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(page_content='8.5.2 Features for CRF Named Entity Recognizers\\n\\nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15.\\n\\nidentity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words presence of wi in a gazetteer wi contains a particular preﬁx (from all preﬁxes of length ≤ 4) wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4) word shape of wi, word shape of neighboring words short word shape of wi, short word shape of neighboring words gazetteer features Figure 8.15 Typical features for a feature-based NER system.'), Document(page_content='The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set.\\n\\nRemember that in a CRF we don’t learn weights for each of these local features fk. Instead, we ﬁrst sum the values of each local feature (for example feature f3743) over the entire sentence, to create each global feature (for example F3743). It is those global features that will then be multiplied by weight w3743. Thus for training and inference there is always a ﬁxed set of K features with K weights, even though the length of each sentence is different.\\n\\n8.5.2 Features for CRF Named Entity Recognizers'), Document(page_content='8.3 Named Entities and Named Entity Tagging\\n\\nPart of speech tagging can tell us that words like Janet, Stanford University, and Colorado are all proper nouns; being a proper noun is a grammatical property of these words. But viewed from a semantic perspective, these proper nouns refer to different kinds of entities: Janet is a person, Stanford University is an organization, and Colorado is a location.\\n\\nnamed entity\\n\\nnamed entity recognition NER'), Document(page_content='Nonetheless, many words are easy to disambiguate, because their different tags aren’t equally likely. For example, a can be a determiner or the letter a, but the determiner sense is much more likely.\\n\\nThis idea suggests a useful baseline: given an ambiguous word, choose the tag\\n\\nwhich is most frequent in the training corpus. This is a key concept:\\n\\nMost Frequent Class Baseline: Always compare a classiﬁer against a baseline at least as good as the most frequent class baseline (assigning each token to the class it occurred in most often in the training set).\\n\\n6 CHAPTER 8\\n\\nSEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\\n\\nThe most-frequent-tag baseline has an accuracy of about 92%1. The baseline\\n\\nthus differs from the state-of-the-art and human ceiling (97%) by only 5%.\\n\\n8.3 Named Entities and Named Entity Tagging')]\n",
            "[Document(page_content='(cid:80)K\\n\\n(where c is the correct class)\\n\\n(7.26)\\n\\n7.5.2 Computing the Gradient\\n\\nHow do we compute the gradient of this loss function? Computing the gradient requires the partial derivative of the loss function with respect to each parameter. For a network with one weight layer and sigmoid output (which is what logistic regression is), we could simply use the derivative of the loss that we used for logistic regression in Eq. 7.27 (and derived in Section ??):\\n\\n∂ LCE (ˆy, y) ∂ w j\\n\\n= ( ˆy − y) x j\\n\\n= (σ (w · x + b) − y) x j\\n\\n(7.27)\\n\\nOr for a network with one weight layer and softmax output (=multinomial logistic regression), we could use the derivative of the softmax loss from Eq. ??, shown for\\n\\n16 CHAPTER 7\\n\\nerror back- propagation\\n\\nchain rule\\n\\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\\n\\na particular weight wk and input xi\\n\\n∂ LCE(ˆy, y) ∂ wk,i\\n\\n= −(yk − ˆyk)xi\\n\\n= −(yk − p(yk = 1|x))xi\\n\\n= −\\n\\n(cid:32)\\n\\nyk −\\n\\nexp (wk · x + bk) j=1 exp (wj · x + b j)\\n\\n(cid:80)K\\n\\n(cid:33)\\n\\nxi'), Document(page_content='h1 head\\n\\nFFN\\n\\nENCODER\\n\\nhead\\n\\nW\\n\\nscore(h1head, h3dep)\\n\\nFigure 18.14 Computing scores for a single edge (book→ ﬂight) in the biafﬁne parser of Dozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward net- works to turn the encoder output for each word into a head and dependent representation for the word. The biafﬁne function turns the head embedding of the head and the dependent embedding of the dependent into a score for the dependency edge.'), Document(page_content='exp(s(i, yi)) y(cid:48)∈Y (i) exp(s(i, y(cid:48)))\\n\\n(22.48)\\n\\nThis score s(i, j) includes three factors that we’ll deﬁne below: m(i); whether span i is a mention; m( j); whether span j is a mention; and c(i, j); whether j is the antecedent of i:\\n\\ns(i, j) = m(i) + m( j) + c(i, j)\\n\\n(22.49)\\n\\nFor the dummy antecedent (cid:15), the score s(i, (cid:15)) is ﬁxed to 0. This way if any non- dummy scores are positive, the model predicts the highest-scoring antecedent, but if all the scores are negative it abstains.\\n\\n22.6.1 Computing span representations'), Document(page_content='Fig. 23.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 23.9. There is one row for each of the six sentences. The second column, for the entity ‘trial’, is O – – – X, showing that the trial appears in the ﬁrst sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as sub- ject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical func- tion). Computing the entity grids requires extracting entities and doing coreference\\n\\nBarzilayandLapataModelingLocalCoherence')]\n",
            "[Document(page_content='9.8\\n\\nimpression 5.95 3.65 0.98\\n\\nagreement 0.3\\n\\nWord Relatedness The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness (Budanitsky and Hirst, 2006), also traditionally called word association in psychology.\\n\\nConsider the meanings of the words coffee and cup. Coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).\\n\\nOne common kind of relatedness between words is if they belong to the same semantic ﬁeld. A semantic ﬁeld is a set of words which cover a particular semantic domain and bear structured relations with each other. For example, words might be\\n\\n4 CHAPTER 6'), Document(page_content='8.5.2 Features for CRF Named Entity Recognizers\\n\\nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15.\\n\\nidentity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words presence of wi in a gazetteer wi contains a particular preﬁx (from all preﬁxes of length ≤ 4) wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4) word shape of wi, word shape of neighboring words short word shape of wi, short word shape of neighboring words gazetteer features Figure 8.15 Typical features for a feature-based NER system.'), Document(page_content='parts of speech\\n\\nDionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long time ago), wrote a grammatical sketch of Greek (a “techn¯e”) that summarized the linguistic knowledge of his day. This work is the source of an astonishing proportion of modern linguistic vocabulary, including the words syntax, diphthong, clitic, and analogy. Also included are a description of eight parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Although earlier scholars (including Aristotle as well as the Stoics) had their own lists of parts of speech, it was Thrax’s set of eight that became the basis for descriptions of European languages for the next 2000 years. (All the way to the Schoolhouse Rock educational television shows of our childhood, which had songs about 8 parts of speech, like the late great Bob Dorough’s Conjunction Junction.) The durability of parts of speech through two millennia speaks to their centrality in models of human language.'), Document(page_content='First or last word (or embedding) of antecedent/anaphor Head word (or head embedding) of antecedent/anaphor The number, gender, animacy, person, named entity type attributes of (antecedent/anaphor) length in words of (antecedent/anaphor) Type: (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun) of an- tecedent/anaphor Features of the Antecedent Entity\\n\\nEntity shape\\n\\nP-Pr-D\\n\\nThe ‘shape’ or list of types of the mentions in the antecedent entity (cluster), i.e., sequences of (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun.\\n\\nEntity attributes\\n\\nSg-F-A-3-PER The number, gender, animacy, person, named entity type\\n\\nAnt. cluster size\\n\\n3\\n\\nattributes of the antecedent entity Number of mentions in the antecedent cluster\\n\\nFeatures of the Pair of Mentions\\n\\nSentence distance Mention distance i-within-i Cosine\\n\\n1 4 F\\n\\nThe number of sentences between antecedent and anaphor The number of mentions between antecedent and anaphor Anaphor has i-within-i relation with antecedent Cosine between antecedent and anaphor embeddings')]\n"
          ]
        }
      ],
      "source": [
        "print(bm25_retriever.invoke(related_query))\n",
        "print(bm25_retriever.invoke(unrelated_query))\n",
        "print(bm25_retriever.invoke(super_unrelated_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ORfRrSKFQN9H"
      },
      "outputs": [],
      "source": [
        "def build_ensemble_retriever(wights):\n",
        "    return EnsembleRetriever(\n",
        "        retrievers=[bm25_retriever, faiss_retriever], wights=wights\n",
        "    )\n",
        "\n",
        "def test_ensemble_retriever(wights):\n",
        "    ensemble_retriever = build_ensemble_retriever(wights)\n",
        "    print(f\"for {wights}:\")\n",
        "    print(ensemble_retriever.invoke(related_query))\n",
        "    print(ensemble_retriever.invoke(unrelated_query))\n",
        "    print(ensemble_retriever.invoke(super_unrelated_query))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD3F1wYvQk6R",
        "outputId": "1ebffd94-6268-4469-bdab-77b14cac4a8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for [0.5, 0.5]:\n",
            "[Document(page_content='8.5.2 Features for CRF Named Entity Recognizers\\n\\nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15.\\n\\nidentity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words presence of wi in a gazetteer wi contains a particular preﬁx (from all preﬁxes of length ≤ 4) wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4) word shape of wi, word shape of neighboring words short word shape of wi, short word shape of neighboring words gazetteer features Figure 8.15 Typical features for a feature-based NER system.'), Document(metadata={'source': '/tmp/tmpz3h9atcu/tmp.pdf'}, page_content='named entity\\n\\nnamed entity recognition NER\\n\\nA named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recog- nition (NER) is to ﬁnd spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren’t entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here’s an example of the output of an NER tagger:'), Document(page_content='The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set.\\n\\nRemember that in a CRF we don’t learn weights for each of these local features fk. Instead, we ﬁrst sum the values of each local feature (for example feature f3743) over the entire sentence, to create each global feature (for example F3743). It is those global features that will then be multiplied by weight w3743. Thus for training and inference there is always a ﬁxed set of K features with K weights, even though the length of each sentence is different.\\n\\n8.5.2 Features for CRF Named Entity Recognizers'), Document(metadata={'source': '/tmp/tmpz3h9atcu/tmp.pdf'}, page_content='8.6 Evaluation of Named Entity Recognition\\n\\nPart-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F-measure is the harmonic mean of the two.\\n\\nTo know if the difference between the F1 scores of two NER systems is a signif- icant difference, we use the paired bootstrap test, or the similar randomization test (Section ??).\\n\\nFor named entity tagging, the entity rather than the word is the unit of response. Thus in the example in Fig. 8.16, the two entities Jane Villanueva and United Air- lines Holding and the non-entity discussed would each count as a single response.'), Document(page_content='8.3 Named Entities and Named Entity Tagging\\n\\nPart of speech tagging can tell us that words like Janet, Stanford University, and Colorado are all proper nouns; being a proper noun is a grammatical property of these words. But viewed from a semantic perspective, these proper nouns refer to different kinds of entities: Janet is a person, Stanford University is an organization, and Colorado is a location.\\n\\nnamed entity\\n\\nnamed entity recognition NER'), Document(page_content='Nonetheless, many words are easy to disambiguate, because their different tags aren’t equally likely. For example, a can be a determiner or the letter a, but the determiner sense is much more likely.\\n\\nThis idea suggests a useful baseline: given an ambiguous word, choose the tag\\n\\nwhich is most frequent in the training corpus. This is a key concept:\\n\\nMost Frequent Class Baseline: Always compare a classiﬁer against a baseline at least as good as the most frequent class baseline (assigning each token to the class it occurred in most often in the training set).\\n\\n6 CHAPTER 8\\n\\nSEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\\n\\nThe most-frequent-tag baseline has an accuracy of about 92%1. The baseline\\n\\nthus differs from the state-of-the-art and human ceiling (97%) by only 5%.\\n\\n8.3 Named Entities and Named Entity Tagging'), Document(metadata={'source': '/tmp/tmpdc4enbfj/tmp.pdf'}, page_content='[CLS]Janetwillbackthebill\\n\\nEmbeddingLayer\\n\\nNNPMDVBDTNN\\n\\nBidirectional Transformer Encoder\\n\\nFigure 11.11 Sequence labeling for part-of-speech tagging with a bidirectional transformer encoder. The output vector for each input token is passed to a simple k-way classiﬁer.\\n\\nA complication with this approach arises from the use of subword tokenization such as WordPiece, SentencePiece Unigram LM or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:\\n\\n(11.14)\\n\\n(11.15)\\n\\n11.5\\n\\nADVANCED: SPAN-BASED MASKING 17\\n\\n[LOC Mt. Sanitas ] is in [LOC Sunshine Canyon] .\\n\\nwould have the following set of per-word BIO tags.\\n\\n(11.16) Mt.\\n\\nB-LOC\\n\\nSanitas I-LOC\\n\\nis O\\n\\nin O\\n\\nSunshine B-LOC\\n\\nCanyon I-LOC\\n\\n. O')]\n",
            "[Document(page_content='(cid:80)K\\n\\n(where c is the correct class)\\n\\n(7.26)\\n\\n7.5.2 Computing the Gradient\\n\\nHow do we compute the gradient of this loss function? Computing the gradient requires the partial derivative of the loss function with respect to each parameter. For a network with one weight layer and sigmoid output (which is what logistic regression is), we could simply use the derivative of the loss that we used for logistic regression in Eq. 7.27 (and derived in Section ??):\\n\\n∂ LCE (ˆy, y) ∂ w j\\n\\n= ( ˆy − y) x j\\n\\n= (σ (w · x + b) − y) x j\\n\\n(7.27)\\n\\nOr for a network with one weight layer and softmax output (=multinomial logistic regression), we could use the derivative of the softmax loss from Eq. ??, shown for\\n\\n16 CHAPTER 7\\n\\nerror back- propagation\\n\\nchain rule\\n\\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\\n\\na particular weight wk and input xi\\n\\n∂ LCE(ˆy, y) ∂ wk,i\\n\\n= −(yk − ˆyk)xi\\n\\n= −(yk − p(yk = 1|x))xi\\n\\n= −\\n\\n(cid:32)\\n\\nyk −\\n\\nexp (wk · x + bk) j=1 exp (wj · x + b j)\\n\\n(cid:80)K\\n\\n(cid:33)\\n\\nxi'), Document(metadata={'source': '/tmp/tmp8h47rjv0/tmp.pdf'}, page_content='18 CHAPTER 15\\n\\nCHATBOTS & DIALOGUE SYSTEMS\\n\\nthink is a program but is in fact a human “wizard” disguised by a software interface (Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name comes from the chil- dren’s book The Wizard of Oz (Baum, 1900), in which the wizard turned out to be a simu- lation controlled by a man behind a curtain or screen. A wizard system can be used to test out an architecture before implementation; only the interface software and databases need to be in place. The wizard gets input from the user, uses a database interface to run queries based on the user utterance, and then outputs sentences, ei- ther by typing them or speaking them.\\n\\nWizard-of-Oz systems are not a perfect simulation, since the wizard doesn’t exactly simulate the errors or limitations of a real sys- tem; but wizard studies can still provide a useful ﬁrst idea of the domain issues.\\n\\nbarged in\\n\\nvalue sensitive design'), Document(page_content='h1 head\\n\\nFFN\\n\\nENCODER\\n\\nhead\\n\\nW\\n\\nscore(h1head, h3dep)\\n\\nFigure 18.14 Computing scores for a single edge (book→ ﬂight) in the biafﬁne parser of Dozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward net- works to turn the encoder output for each word into a head and dependent representation for the word. The biafﬁne function turns the head embedding of the head and the dependent embedding of the dependent into a score for the dependency edge.'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='Basing our answers on retrieved documents can solve the above-mentioned prob- lems with using simple prompting to answer questions. First, we can ensure that the answer is grounded in facts from some curated dataset. And we can give the answer accompanied by the context of the passage or document the answer came from. This information can help users have conﬁdence in the accuracy of the answer (or help them spot when it is wrong!). And we can use our retrieval techniques on any pro- prietary data we want, such as legal or medical data for those applications.'), Document(page_content='exp(s(i, yi)) y(cid:48)∈Y (i) exp(s(i, y(cid:48)))\\n\\n(22.48)\\n\\nThis score s(i, j) includes three factors that we’ll deﬁne below: m(i); whether span i is a mention; m( j); whether span j is a mention; and c(i, j); whether j is the antecedent of i:\\n\\ns(i, j) = m(i) + m( j) + c(i, j)\\n\\n(22.49)\\n\\nFor the dummy antecedent (cid:15), the score s(i, (cid:15)) is ﬁxed to 0. This way if any non- dummy scores are positive, the model predicts the highest-scoring antecedent, but if all the scores are negative it abstains.\\n\\n22.6.1 Computing span representations'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='20 CHAPTER 14\\n\\nQUESTION ANSWERING AND INFORMATION RETRIEVAL\\n\\nThe match between a query and a document can be done by ﬁrst representing each of them with a sparse vector that represents the frequencies of words, weighted by tf-idf or BM25. Then the similarity can be measured by cosine. • Documents or queries can instead be represented by dense vectors, by encod- ing the question and document with an encoder-only model like BERT, and in that case computing similarity in embedding space.\\n\\nThe inverted index is an storage mechanism that makes it very efﬁcient to ﬁnd documents that have a particular word.\\n\\nRanked retrieval is generally evaluated by mean average precision or inter- polated precision.\\n\\nQuestion answering systems generally use the retriever/reader architecture. In the retriever stage, an IR system is given a query and returns a set of documents.'), Document(page_content='Fig. 23.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 23.9. There is one row for each of the six sentences. The second column, for the entity ‘trial’, is O – – – X, showing that the trial appears in the ﬁrst sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as sub- ject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical func- tion). Computing the entity grids requires extracting entities and doing coreference\\n\\nBarzilayandLapataModelingLocalCoherence'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='Speech and Language Processing. Daniel Jurafsky & James H. Martin. rights reserved.\\n\\nDraft of February 3, 2024.\\n\\nCopyright © 2023.\\n\\nCHAPTER\\n\\n14 Question Answering and In-\\n\\nformation Retrieval\\n\\nThe quest for knowledge is deeply human, and so it is not surprising that practically as soon as there were computers we were asking them questions. By the early 1960s, systems used the two major paradigms of question answering—retrieval-based and knowledge-based—to answer questions about baseball statistics or scientiﬁc facts. Even imaginary computers got into the act. Deep Thought, the computer that Dou- glas Adams invented in The Hitchhiker’s Guide to the Galaxy, managed to answer “the Ultimate Question Of Life, The Universe, and Everything”.1 In 2011, IBM’s Watson question-answering system won the TV game-show Jeopardy!, surpassing humans at answering questions like:\\n\\nWILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA”INSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL\\n\\n2')]\n",
            "[Document(page_content='9.8\\n\\nimpression 5.95 3.65 0.98\\n\\nagreement 0.3\\n\\nWord Relatedness The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness (Budanitsky and Hirst, 2006), also traditionally called word association in psychology.\\n\\nConsider the meanings of the words coffee and cup. Coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).\\n\\nOne common kind of relatedness between words is if they belong to the same semantic ﬁeld. A semantic ﬁeld is a set of words which cover a particular semantic domain and bear structured relations with each other. For example, words might be\\n\\n4 CHAPTER 6'), Document(metadata={'source': '/tmp/tmp16hw7jef/tmp.pdf'}, page_content='Sometime between the 7th and 4th centuries BCE, the Indian grammarian P¯an. ini1 wrote a famous treatise on Sanskrit grammar, the As.t.¯adhy¯ay¯ı (‘8 books’), a treatise that has been called “one of the greatest monuments of hu- man intelligence” (Bloomﬁeld, 1933, 11). The work de- scribes the linguistics of the Sanskrit language in the form of 3959 sutras, each very efﬁciently (since it had to be memorized!) expressing part of a formal rule system that brilliantly preﬁgured modern mechanisms of formal lan- guage theory (Penn and Kiparsky, 2012). One set of rules describes the k¯arakas, semantic relationships between a verb and noun arguments, roles like agent, instrument, or destination. P¯an. ini’s work was the earliest we know of that modeled the linguistic realization of events and their participants. This task of understanding how participants relate to events—being able to answer the question “Who did what to whom” (and perhaps also “when and where”)—is a central question of natural language'), Document(page_content='8.5.2 Features for CRF Named Entity Recognizers\\n\\nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15.\\n\\nidentity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words presence of wi in a gazetteer wi contains a particular preﬁx (from all preﬁxes of length ≤ 4) wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4) word shape of wi, word shape of neighboring words short word shape of wi, short word shape of neighboring words gazetteer features Figure 8.15 Typical features for a feature-based NER system.'), Document(metadata={'source': '/tmp/tmpkxwhtz_7/tmp.pdf'}, page_content='(13.1) English: He wrote a letter to a friend\\n\\nJapanese:\\n\\ntomodachi friend\\n\\nni to\\n\\ntegami-o letter\\n\\nkaita wrote\\n\\nNote that the elements of the sentences are in very different places in the different languages. In English, the verb is in the middle of the sentence, while in Japanese, the verb kaita comes at the end. The Japanese sentence doesn’t require the pronoun he, while English does.\\n\\nSuch differences between languages can be quite complex. In the following ac- tual sentence from the United Nations, notice the many changes between the Chinese sentence (we’ve given in red a word-by-word gloss of the Chinese characters) and its English equivalent produced by human translators. (13.2) 大会/General Assembly 在/on 1982年/1982 12月/December 10日/10 通过 了/adopted 第37号/37th 决议/resolution ，核准了/approved 第二 次/second 探索/exploration 及/and 和平peaceful colorblue利用/using 外 } 层空间/outer space 会议/conference 的/of 各项/various 建议/suggestions 。'), Document(page_content='parts of speech\\n\\nDionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long time ago), wrote a grammatical sketch of Greek (a “techn¯e”) that summarized the linguistic knowledge of his day. This work is the source of an astonishing proportion of modern linguistic vocabulary, including the words syntax, diphthong, clitic, and analogy. Also included are a description of eight parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Although earlier scholars (including Aristotle as well as the Stoics) had their own lists of parts of speech, it was Thrax’s set of eight that became the basis for descriptions of European languages for the next 2000 years. (All the way to the Schoolhouse Rock educational television shows of our childhood, which had songs about 8 parts of speech, like the late great Bob Dorough’s Conjunction Junction.) The durability of parts of speech through two millennia speaks to their centrality in models of human language.'), Document(metadata={'source': '/tmp/tmp16hw7jef/tmp.pdf'}, page_content='In this chapter we introduce a level of representation that captures the common- ality between these sentences: there was a purchase event, the participants were XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic representations , semantic roles, express the role that arguments of a predicate take in the event, codiﬁed in databases like PropBank and FrameNet. We’ll introduce semantic role labeling, the task of assigning roles to spans in sentences, and selec- tional restrictions, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible.\\n\\n1 Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based on the Sanskrit grammar of Panini. Image from the Wellcome Collection.\\n\\n2 CHAPTER 20\\n\\nSEMANTIC ROLE LABELING\\n\\n20.1 Semantic Roles'), Document(page_content='First or last word (or embedding) of antecedent/anaphor Head word (or head embedding) of antecedent/anaphor The number, gender, animacy, person, named entity type attributes of (antecedent/anaphor) length in words of (antecedent/anaphor) Type: (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun) of an- tecedent/anaphor Features of the Antecedent Entity\\n\\nEntity shape\\n\\nP-Pr-D\\n\\nThe ‘shape’ or list of types of the mentions in the antecedent entity (cluster), i.e., sequences of (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun.\\n\\nEntity attributes\\n\\nSg-F-A-3-PER The number, gender, animacy, person, named entity type\\n\\nAnt. cluster size\\n\\n3\\n\\nattributes of the antecedent entity Number of mentions in the antecedent cluster\\n\\nFeatures of the Pair of Mentions\\n\\nSentence distance Mention distance i-within-i Cosine\\n\\n1 4 F\\n\\nThe number of sentences between antecedent and anaphor The number of mentions between antecedent and anaphor Anaphor has i-within-i relation with antecedent Cosine between antecedent and anaphor embeddings'), Document(metadata={'source': '/tmp/tmpnpvt6que/tmp.pdf'}, page_content='parsing. ACL.\\n\\nNaur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz, J. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson, B. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and M. Woodger. 1960. Report on the algorithmic language ALGOL 60. CACM, 3(5):299–314. Revised in CACM 6:1, 1-17, 1963.\\n\\nNorvig, P. 1991. Techniques for automatic memoization with applications to context-free parsing. Computational Lin- guistics, 17(1):91–98.\\n\\nPercival, W. K. 1976. On the historical source of immediate constituent analysis. In J. D. McCawley, editor, Syntax and Semantics Volume 7, Notes from the Linguistic Un- derground, pages 229–242. Academic Press.\\n\\nPollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\\n\\nture Grammar. University of Chicago Press.\\n\\nSalomaa, A. 1969. Probabilistic and weighted grammars.\\n\\nInformation and Control, 15:529–544.\\n\\nSekine, S. and M. Collins. 1997. The evalb software. http:\\n\\n//cs.nyu.edu/cs/projects/proteus/evalb.\\n\\n25\\n\\n26 Chapter 17\\n\\nContext-Free Grammars and Constituency Parsing')]\n",
            "\n",
            "\n",
            "for [0.1, 0.9]:\n",
            "[Document(page_content='8.5.2 Features for CRF Named Entity Recognizers\\n\\nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15.\\n\\nidentity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words presence of wi in a gazetteer wi contains a particular preﬁx (from all preﬁxes of length ≤ 4) wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4) word shape of wi, word shape of neighboring words short word shape of wi, short word shape of neighboring words gazetteer features Figure 8.15 Typical features for a feature-based NER system.'), Document(metadata={'source': '/tmp/tmpz3h9atcu/tmp.pdf'}, page_content='named entity\\n\\nnamed entity recognition NER\\n\\nA named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recog- nition (NER) is to ﬁnd spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren’t entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here’s an example of the output of an NER tagger:'), Document(page_content='The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set.\\n\\nRemember that in a CRF we don’t learn weights for each of these local features fk. Instead, we ﬁrst sum the values of each local feature (for example feature f3743) over the entire sentence, to create each global feature (for example F3743). It is those global features that will then be multiplied by weight w3743. Thus for training and inference there is always a ﬁxed set of K features with K weights, even though the length of each sentence is different.\\n\\n8.5.2 Features for CRF Named Entity Recognizers'), Document(metadata={'source': '/tmp/tmpz3h9atcu/tmp.pdf'}, page_content='8.6 Evaluation of Named Entity Recognition\\n\\nPart-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F-measure is the harmonic mean of the two.\\n\\nTo know if the difference between the F1 scores of two NER systems is a signif- icant difference, we use the paired bootstrap test, or the similar randomization test (Section ??).\\n\\nFor named entity tagging, the entity rather than the word is the unit of response. Thus in the example in Fig. 8.16, the two entities Jane Villanueva and United Air- lines Holding and the non-entity discussed would each count as a single response.'), Document(page_content='8.3 Named Entities and Named Entity Tagging\\n\\nPart of speech tagging can tell us that words like Janet, Stanford University, and Colorado are all proper nouns; being a proper noun is a grammatical property of these words. But viewed from a semantic perspective, these proper nouns refer to different kinds of entities: Janet is a person, Stanford University is an organization, and Colorado is a location.\\n\\nnamed entity\\n\\nnamed entity recognition NER'), Document(page_content='Nonetheless, many words are easy to disambiguate, because their different tags aren’t equally likely. For example, a can be a determiner or the letter a, but the determiner sense is much more likely.\\n\\nThis idea suggests a useful baseline: given an ambiguous word, choose the tag\\n\\nwhich is most frequent in the training corpus. This is a key concept:\\n\\nMost Frequent Class Baseline: Always compare a classiﬁer against a baseline at least as good as the most frequent class baseline (assigning each token to the class it occurred in most often in the training set).\\n\\n6 CHAPTER 8\\n\\nSEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\\n\\nThe most-frequent-tag baseline has an accuracy of about 92%1. The baseline\\n\\nthus differs from the state-of-the-art and human ceiling (97%) by only 5%.\\n\\n8.3 Named Entities and Named Entity Tagging'), Document(metadata={'source': '/tmp/tmpdc4enbfj/tmp.pdf'}, page_content='[CLS]Janetwillbackthebill\\n\\nEmbeddingLayer\\n\\nNNPMDVBDTNN\\n\\nBidirectional Transformer Encoder\\n\\nFigure 11.11 Sequence labeling for part-of-speech tagging with a bidirectional transformer encoder. The output vector for each input token is passed to a simple k-way classiﬁer.\\n\\nA complication with this approach arises from the use of subword tokenization such as WordPiece, SentencePiece Unigram LM or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:\\n\\n(11.14)\\n\\n(11.15)\\n\\n11.5\\n\\nADVANCED: SPAN-BASED MASKING 17\\n\\n[LOC Mt. Sanitas ] is in [LOC Sunshine Canyon] .\\n\\nwould have the following set of per-word BIO tags.\\n\\n(11.16) Mt.\\n\\nB-LOC\\n\\nSanitas I-LOC\\n\\nis O\\n\\nin O\\n\\nSunshine B-LOC\\n\\nCanyon I-LOC\\n\\n. O')]\n",
            "[Document(page_content='(cid:80)K\\n\\n(where c is the correct class)\\n\\n(7.26)\\n\\n7.5.2 Computing the Gradient\\n\\nHow do we compute the gradient of this loss function? Computing the gradient requires the partial derivative of the loss function with respect to each parameter. For a network with one weight layer and sigmoid output (which is what logistic regression is), we could simply use the derivative of the loss that we used for logistic regression in Eq. 7.27 (and derived in Section ??):\\n\\n∂ LCE (ˆy, y) ∂ w j\\n\\n= ( ˆy − y) x j\\n\\n= (σ (w · x + b) − y) x j\\n\\n(7.27)\\n\\nOr for a network with one weight layer and softmax output (=multinomial logistic regression), we could use the derivative of the softmax loss from Eq. ??, shown for\\n\\n16 CHAPTER 7\\n\\nerror back- propagation\\n\\nchain rule\\n\\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\\n\\na particular weight wk and input xi\\n\\n∂ LCE(ˆy, y) ∂ wk,i\\n\\n= −(yk − ˆyk)xi\\n\\n= −(yk − p(yk = 1|x))xi\\n\\n= −\\n\\n(cid:32)\\n\\nyk −\\n\\nexp (wk · x + bk) j=1 exp (wj · x + b j)\\n\\n(cid:80)K\\n\\n(cid:33)\\n\\nxi'), Document(metadata={'source': '/tmp/tmp8h47rjv0/tmp.pdf'}, page_content='18 CHAPTER 15\\n\\nCHATBOTS & DIALOGUE SYSTEMS\\n\\nthink is a program but is in fact a human “wizard” disguised by a software interface (Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name comes from the chil- dren’s book The Wizard of Oz (Baum, 1900), in which the wizard turned out to be a simu- lation controlled by a man behind a curtain or screen. A wizard system can be used to test out an architecture before implementation; only the interface software and databases need to be in place. The wizard gets input from the user, uses a database interface to run queries based on the user utterance, and then outputs sentences, ei- ther by typing them or speaking them.\\n\\nWizard-of-Oz systems are not a perfect simulation, since the wizard doesn’t exactly simulate the errors or limitations of a real sys- tem; but wizard studies can still provide a useful ﬁrst idea of the domain issues.\\n\\nbarged in\\n\\nvalue sensitive design'), Document(page_content='h1 head\\n\\nFFN\\n\\nENCODER\\n\\nhead\\n\\nW\\n\\nscore(h1head, h3dep)\\n\\nFigure 18.14 Computing scores for a single edge (book→ ﬂight) in the biafﬁne parser of Dozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward net- works to turn the encoder output for each word into a head and dependent representation for the word. The biafﬁne function turns the head embedding of the head and the dependent embedding of the dependent into a score for the dependency edge.'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='Basing our answers on retrieved documents can solve the above-mentioned prob- lems with using simple prompting to answer questions. First, we can ensure that the answer is grounded in facts from some curated dataset. And we can give the answer accompanied by the context of the passage or document the answer came from. This information can help users have conﬁdence in the accuracy of the answer (or help them spot when it is wrong!). And we can use our retrieval techniques on any pro- prietary data we want, such as legal or medical data for those applications.'), Document(page_content='exp(s(i, yi)) y(cid:48)∈Y (i) exp(s(i, y(cid:48)))\\n\\n(22.48)\\n\\nThis score s(i, j) includes three factors that we’ll deﬁne below: m(i); whether span i is a mention; m( j); whether span j is a mention; and c(i, j); whether j is the antecedent of i:\\n\\ns(i, j) = m(i) + m( j) + c(i, j)\\n\\n(22.49)\\n\\nFor the dummy antecedent (cid:15), the score s(i, (cid:15)) is ﬁxed to 0. This way if any non- dummy scores are positive, the model predicts the highest-scoring antecedent, but if all the scores are negative it abstains.\\n\\n22.6.1 Computing span representations'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='20 CHAPTER 14\\n\\nQUESTION ANSWERING AND INFORMATION RETRIEVAL\\n\\nThe match between a query and a document can be done by ﬁrst representing each of them with a sparse vector that represents the frequencies of words, weighted by tf-idf or BM25. Then the similarity can be measured by cosine. • Documents or queries can instead be represented by dense vectors, by encod- ing the question and document with an encoder-only model like BERT, and in that case computing similarity in embedding space.\\n\\nThe inverted index is an storage mechanism that makes it very efﬁcient to ﬁnd documents that have a particular word.\\n\\nRanked retrieval is generally evaluated by mean average precision or inter- polated precision.\\n\\nQuestion answering systems generally use the retriever/reader architecture. In the retriever stage, an IR system is given a query and returns a set of documents.'), Document(page_content='Fig. 23.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 23.9. There is one row for each of the six sentences. The second column, for the entity ‘trial’, is O – – – X, showing that the trial appears in the ﬁrst sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as sub- ject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical func- tion). Computing the entity grids requires extracting entities and doing coreference\\n\\nBarzilayandLapataModelingLocalCoherence'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='Speech and Language Processing. Daniel Jurafsky & James H. Martin. rights reserved.\\n\\nDraft of February 3, 2024.\\n\\nCopyright © 2023.\\n\\nCHAPTER\\n\\n14 Question Answering and In-\\n\\nformation Retrieval\\n\\nThe quest for knowledge is deeply human, and so it is not surprising that practically as soon as there were computers we were asking them questions. By the early 1960s, systems used the two major paradigms of question answering—retrieval-based and knowledge-based—to answer questions about baseball statistics or scientiﬁc facts. Even imaginary computers got into the act. Deep Thought, the computer that Dou- glas Adams invented in The Hitchhiker’s Guide to the Galaxy, managed to answer “the Ultimate Question Of Life, The Universe, and Everything”.1 In 2011, IBM’s Watson question-answering system won the TV game-show Jeopardy!, surpassing humans at answering questions like:\\n\\nWILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA”INSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL\\n\\n2')]\n",
            "[Document(page_content='9.8\\n\\nimpression 5.95 3.65 0.98\\n\\nagreement 0.3\\n\\nWord Relatedness The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness (Budanitsky and Hirst, 2006), also traditionally called word association in psychology.\\n\\nConsider the meanings of the words coffee and cup. Coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).\\n\\nOne common kind of relatedness between words is if they belong to the same semantic ﬁeld. A semantic ﬁeld is a set of words which cover a particular semantic domain and bear structured relations with each other. For example, words might be\\n\\n4 CHAPTER 6'), Document(metadata={'source': '/tmp/tmp16hw7jef/tmp.pdf'}, page_content='Sometime between the 7th and 4th centuries BCE, the Indian grammarian P¯an. ini1 wrote a famous treatise on Sanskrit grammar, the As.t.¯adhy¯ay¯ı (‘8 books’), a treatise that has been called “one of the greatest monuments of hu- man intelligence” (Bloomﬁeld, 1933, 11). The work de- scribes the linguistics of the Sanskrit language in the form of 3959 sutras, each very efﬁciently (since it had to be memorized!) expressing part of a formal rule system that brilliantly preﬁgured modern mechanisms of formal lan- guage theory (Penn and Kiparsky, 2012). One set of rules describes the k¯arakas, semantic relationships between a verb and noun arguments, roles like agent, instrument, or destination. P¯an. ini’s work was the earliest we know of that modeled the linguistic realization of events and their participants. This task of understanding how participants relate to events—being able to answer the question “Who did what to whom” (and perhaps also “when and where”)—is a central question of natural language'), Document(page_content='8.5.2 Features for CRF Named Entity Recognizers\\n\\nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15.\\n\\nidentity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words presence of wi in a gazetteer wi contains a particular preﬁx (from all preﬁxes of length ≤ 4) wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4) word shape of wi, word shape of neighboring words short word shape of wi, short word shape of neighboring words gazetteer features Figure 8.15 Typical features for a feature-based NER system.'), Document(metadata={'source': '/tmp/tmpkxwhtz_7/tmp.pdf'}, page_content='(13.1) English: He wrote a letter to a friend\\n\\nJapanese:\\n\\ntomodachi friend\\n\\nni to\\n\\ntegami-o letter\\n\\nkaita wrote\\n\\nNote that the elements of the sentences are in very different places in the different languages. In English, the verb is in the middle of the sentence, while in Japanese, the verb kaita comes at the end. The Japanese sentence doesn’t require the pronoun he, while English does.\\n\\nSuch differences between languages can be quite complex. In the following ac- tual sentence from the United Nations, notice the many changes between the Chinese sentence (we’ve given in red a word-by-word gloss of the Chinese characters) and its English equivalent produced by human translators. (13.2) 大会/General Assembly 在/on 1982年/1982 12月/December 10日/10 通过 了/adopted 第37号/37th 决议/resolution ，核准了/approved 第二 次/second 探索/exploration 及/and 和平peaceful colorblue利用/using 外 } 层空间/outer space 会议/conference 的/of 各项/various 建议/suggestions 。'), Document(page_content='parts of speech\\n\\nDionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long time ago), wrote a grammatical sketch of Greek (a “techn¯e”) that summarized the linguistic knowledge of his day. This work is the source of an astonishing proportion of modern linguistic vocabulary, including the words syntax, diphthong, clitic, and analogy. Also included are a description of eight parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Although earlier scholars (including Aristotle as well as the Stoics) had their own lists of parts of speech, it was Thrax’s set of eight that became the basis for descriptions of European languages for the next 2000 years. (All the way to the Schoolhouse Rock educational television shows of our childhood, which had songs about 8 parts of speech, like the late great Bob Dorough’s Conjunction Junction.) The durability of parts of speech through two millennia speaks to their centrality in models of human language.'), Document(metadata={'source': '/tmp/tmp16hw7jef/tmp.pdf'}, page_content='In this chapter we introduce a level of representation that captures the common- ality between these sentences: there was a purchase event, the participants were XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic representations , semantic roles, express the role that arguments of a predicate take in the event, codiﬁed in databases like PropBank and FrameNet. We’ll introduce semantic role labeling, the task of assigning roles to spans in sentences, and selec- tional restrictions, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible.\\n\\n1 Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based on the Sanskrit grammar of Panini. Image from the Wellcome Collection.\\n\\n2 CHAPTER 20\\n\\nSEMANTIC ROLE LABELING\\n\\n20.1 Semantic Roles'), Document(page_content='First or last word (or embedding) of antecedent/anaphor Head word (or head embedding) of antecedent/anaphor The number, gender, animacy, person, named entity type attributes of (antecedent/anaphor) length in words of (antecedent/anaphor) Type: (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun) of an- tecedent/anaphor Features of the Antecedent Entity\\n\\nEntity shape\\n\\nP-Pr-D\\n\\nThe ‘shape’ or list of types of the mentions in the antecedent entity (cluster), i.e., sequences of (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun.\\n\\nEntity attributes\\n\\nSg-F-A-3-PER The number, gender, animacy, person, named entity type\\n\\nAnt. cluster size\\n\\n3\\n\\nattributes of the antecedent entity Number of mentions in the antecedent cluster\\n\\nFeatures of the Pair of Mentions\\n\\nSentence distance Mention distance i-within-i Cosine\\n\\n1 4 F\\n\\nThe number of sentences between antecedent and anaphor The number of mentions between antecedent and anaphor Anaphor has i-within-i relation with antecedent Cosine between antecedent and anaphor embeddings'), Document(metadata={'source': '/tmp/tmpnpvt6que/tmp.pdf'}, page_content='parsing. ACL.\\n\\nNaur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz, J. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson, B. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and M. Woodger. 1960. Report on the algorithmic language ALGOL 60. CACM, 3(5):299–314. Revised in CACM 6:1, 1-17, 1963.\\n\\nNorvig, P. 1991. Techniques for automatic memoization with applications to context-free parsing. Computational Lin- guistics, 17(1):91–98.\\n\\nPercival, W. K. 1976. On the historical source of immediate constituent analysis. In J. D. McCawley, editor, Syntax and Semantics Volume 7, Notes from the Linguistic Un- derground, pages 229–242. Academic Press.\\n\\nPollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\\n\\nture Grammar. University of Chicago Press.\\n\\nSalomaa, A. 1969. Probabilistic and weighted grammars.\\n\\nInformation and Control, 15:529–544.\\n\\nSekine, S. and M. Collins. 1997. The evalb software. http:\\n\\n//cs.nyu.edu/cs/projects/proteus/evalb.\\n\\n25\\n\\n26 Chapter 17\\n\\nContext-Free Grammars and Constituency Parsing')]\n",
            "\n",
            "\n",
            "for [0.9, 0.1]:\n",
            "[Document(page_content='8.5.2 Features for CRF Named Entity Recognizers\\n\\nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15.\\n\\nidentity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words presence of wi in a gazetteer wi contains a particular preﬁx (from all preﬁxes of length ≤ 4) wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4) word shape of wi, word shape of neighboring words short word shape of wi, short word shape of neighboring words gazetteer features Figure 8.15 Typical features for a feature-based NER system.'), Document(metadata={'source': '/tmp/tmpz3h9atcu/tmp.pdf'}, page_content='named entity\\n\\nnamed entity recognition NER\\n\\nA named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recog- nition (NER) is to ﬁnd spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren’t entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here’s an example of the output of an NER tagger:'), Document(page_content='The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count < 5 in the training set.\\n\\nRemember that in a CRF we don’t learn weights for each of these local features fk. Instead, we ﬁrst sum the values of each local feature (for example feature f3743) over the entire sentence, to create each global feature (for example F3743). It is those global features that will then be multiplied by weight w3743. Thus for training and inference there is always a ﬁxed set of K features with K weights, even though the length of each sentence is different.\\n\\n8.5.2 Features for CRF Named Entity Recognizers'), Document(metadata={'source': '/tmp/tmpz3h9atcu/tmp.pdf'}, page_content='8.6 Evaluation of Named Entity Recognition\\n\\nPart-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and F1 measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and F-measure is the harmonic mean of the two.\\n\\nTo know if the difference between the F1 scores of two NER systems is a signif- icant difference, we use the paired bootstrap test, or the similar randomization test (Section ??).\\n\\nFor named entity tagging, the entity rather than the word is the unit of response. Thus in the example in Fig. 8.16, the two entities Jane Villanueva and United Air- lines Holding and the non-entity discussed would each count as a single response.'), Document(page_content='8.3 Named Entities and Named Entity Tagging\\n\\nPart of speech tagging can tell us that words like Janet, Stanford University, and Colorado are all proper nouns; being a proper noun is a grammatical property of these words. But viewed from a semantic perspective, these proper nouns refer to different kinds of entities: Janet is a person, Stanford University is an organization, and Colorado is a location.\\n\\nnamed entity\\n\\nnamed entity recognition NER'), Document(page_content='Nonetheless, many words are easy to disambiguate, because their different tags aren’t equally likely. For example, a can be a determiner or the letter a, but the determiner sense is much more likely.\\n\\nThis idea suggests a useful baseline: given an ambiguous word, choose the tag\\n\\nwhich is most frequent in the training corpus. This is a key concept:\\n\\nMost Frequent Class Baseline: Always compare a classiﬁer against a baseline at least as good as the most frequent class baseline (assigning each token to the class it occurred in most often in the training set).\\n\\n6 CHAPTER 8\\n\\nSEQUENCE LABELING FOR PARTS OF SPEECH AND NAMED ENTITIES\\n\\nThe most-frequent-tag baseline has an accuracy of about 92%1. The baseline\\n\\nthus differs from the state-of-the-art and human ceiling (97%) by only 5%.\\n\\n8.3 Named Entities and Named Entity Tagging'), Document(metadata={'source': '/tmp/tmpdc4enbfj/tmp.pdf'}, page_content='[CLS]Janetwillbackthebill\\n\\nEmbeddingLayer\\n\\nNNPMDVBDTNN\\n\\nBidirectional Transformer Encoder\\n\\nFigure 11.11 Sequence labeling for part-of-speech tagging with a bidirectional transformer encoder. The output vector for each input token is passed to a simple k-way classiﬁer.\\n\\nA complication with this approach arises from the use of subword tokenization such as WordPiece, SentencePiece Unigram LM or Byte Pair Encoding. Supervised training data for tasks like named entity recognition (NER) is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:\\n\\n(11.14)\\n\\n(11.15)\\n\\n11.5\\n\\nADVANCED: SPAN-BASED MASKING 17\\n\\n[LOC Mt. Sanitas ] is in [LOC Sunshine Canyon] .\\n\\nwould have the following set of per-word BIO tags.\\n\\n(11.16) Mt.\\n\\nB-LOC\\n\\nSanitas I-LOC\\n\\nis O\\n\\nin O\\n\\nSunshine B-LOC\\n\\nCanyon I-LOC\\n\\n. O')]\n",
            "[Document(page_content='(cid:80)K\\n\\n(where c is the correct class)\\n\\n(7.26)\\n\\n7.5.2 Computing the Gradient\\n\\nHow do we compute the gradient of this loss function? Computing the gradient requires the partial derivative of the loss function with respect to each parameter. For a network with one weight layer and sigmoid output (which is what logistic regression is), we could simply use the derivative of the loss that we used for logistic regression in Eq. 7.27 (and derived in Section ??):\\n\\n∂ LCE (ˆy, y) ∂ w j\\n\\n= ( ˆy − y) x j\\n\\n= (σ (w · x + b) − y) x j\\n\\n(7.27)\\n\\nOr for a network with one weight layer and softmax output (=multinomial logistic regression), we could use the derivative of the softmax loss from Eq. ??, shown for\\n\\n16 CHAPTER 7\\n\\nerror back- propagation\\n\\nchain rule\\n\\nNEURAL NETWORKS AND NEURAL LANGUAGE MODELS\\n\\na particular weight wk and input xi\\n\\n∂ LCE(ˆy, y) ∂ wk,i\\n\\n= −(yk − ˆyk)xi\\n\\n= −(yk − p(yk = 1|x))xi\\n\\n= −\\n\\n(cid:32)\\n\\nyk −\\n\\nexp (wk · x + bk) j=1 exp (wj · x + b j)\\n\\n(cid:80)K\\n\\n(cid:33)\\n\\nxi'), Document(metadata={'source': '/tmp/tmp8h47rjv0/tmp.pdf'}, page_content='18 CHAPTER 15\\n\\nCHATBOTS & DIALOGUE SYSTEMS\\n\\nthink is a program but is in fact a human “wizard” disguised by a software interface (Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name comes from the chil- dren’s book The Wizard of Oz (Baum, 1900), in which the wizard turned out to be a simu- lation controlled by a man behind a curtain or screen. A wizard system can be used to test out an architecture before implementation; only the interface software and databases need to be in place. The wizard gets input from the user, uses a database interface to run queries based on the user utterance, and then outputs sentences, ei- ther by typing them or speaking them.\\n\\nWizard-of-Oz systems are not a perfect simulation, since the wizard doesn’t exactly simulate the errors or limitations of a real sys- tem; but wizard studies can still provide a useful ﬁrst idea of the domain issues.\\n\\nbarged in\\n\\nvalue sensitive design'), Document(page_content='h1 head\\n\\nFFN\\n\\nENCODER\\n\\nhead\\n\\nW\\n\\nscore(h1head, h3dep)\\n\\nFigure 18.14 Computing scores for a single edge (book→ ﬂight) in the biafﬁne parser of Dozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward net- works to turn the encoder output for each word into a head and dependent representation for the word. The biafﬁne function turns the head embedding of the head and the dependent embedding of the dependent into a score for the dependency edge.'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='Basing our answers on retrieved documents can solve the above-mentioned prob- lems with using simple prompting to answer questions. First, we can ensure that the answer is grounded in facts from some curated dataset. And we can give the answer accompanied by the context of the passage or document the answer came from. This information can help users have conﬁdence in the accuracy of the answer (or help them spot when it is wrong!). And we can use our retrieval techniques on any pro- prietary data we want, such as legal or medical data for those applications.'), Document(page_content='exp(s(i, yi)) y(cid:48)∈Y (i) exp(s(i, y(cid:48)))\\n\\n(22.48)\\n\\nThis score s(i, j) includes three factors that we’ll deﬁne below: m(i); whether span i is a mention; m( j); whether span j is a mention; and c(i, j); whether j is the antecedent of i:\\n\\ns(i, j) = m(i) + m( j) + c(i, j)\\n\\n(22.49)\\n\\nFor the dummy antecedent (cid:15), the score s(i, (cid:15)) is ﬁxed to 0. This way if any non- dummy scores are positive, the model predicts the highest-scoring antecedent, but if all the scores are negative it abstains.\\n\\n22.6.1 Computing span representations'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='20 CHAPTER 14\\n\\nQUESTION ANSWERING AND INFORMATION RETRIEVAL\\n\\nThe match between a query and a document can be done by ﬁrst representing each of them with a sparse vector that represents the frequencies of words, weighted by tf-idf or BM25. Then the similarity can be measured by cosine. • Documents or queries can instead be represented by dense vectors, by encod- ing the question and document with an encoder-only model like BERT, and in that case computing similarity in embedding space.\\n\\nThe inverted index is an storage mechanism that makes it very efﬁcient to ﬁnd documents that have a particular word.\\n\\nRanked retrieval is generally evaluated by mean average precision or inter- polated precision.\\n\\nQuestion answering systems generally use the retriever/reader architecture. In the retriever stage, an IR system is given a query and returns a set of documents.'), Document(page_content='Fig. 23.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 23.9. There is one row for each of the six sentences. The second column, for the entity ‘trial’, is O – – – X, showing that the trial appears in the ﬁrst sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as sub- ject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical func- tion). Computing the entity grids requires extracting entities and doing coreference\\n\\nBarzilayandLapataModelingLocalCoherence'), Document(metadata={'source': '/tmp/tmpcmslhaof/tmp.pdf'}, page_content='Speech and Language Processing. Daniel Jurafsky & James H. Martin. rights reserved.\\n\\nDraft of February 3, 2024.\\n\\nCopyright © 2023.\\n\\nCHAPTER\\n\\n14 Question Answering and In-\\n\\nformation Retrieval\\n\\nThe quest for knowledge is deeply human, and so it is not surprising that practically as soon as there were computers we were asking them questions. By the early 1960s, systems used the two major paradigms of question answering—retrieval-based and knowledge-based—to answer questions about baseball statistics or scientiﬁc facts. Even imaginary computers got into the act. Deep Thought, the computer that Dou- glas Adams invented in The Hitchhiker’s Guide to the Galaxy, managed to answer “the Ultimate Question Of Life, The Universe, and Everything”.1 In 2011, IBM’s Watson question-answering system won the TV game-show Jeopardy!, surpassing humans at answering questions like:\\n\\nWILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA”INSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL\\n\\n2')]\n",
            "[Document(page_content='9.8\\n\\nimpression 5.95 3.65 0.98\\n\\nagreement 0.3\\n\\nWord Relatedness The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness (Budanitsky and Hirst, 2006), also traditionally called word association in psychology.\\n\\nConsider the meanings of the words coffee and cup. Coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).\\n\\nOne common kind of relatedness between words is if they belong to the same semantic ﬁeld. A semantic ﬁeld is a set of words which cover a particular semantic domain and bear structured relations with each other. For example, words might be\\n\\n4 CHAPTER 6'), Document(metadata={'source': '/tmp/tmp16hw7jef/tmp.pdf'}, page_content='Sometime between the 7th and 4th centuries BCE, the Indian grammarian P¯an. ini1 wrote a famous treatise on Sanskrit grammar, the As.t.¯adhy¯ay¯ı (‘8 books’), a treatise that has been called “one of the greatest monuments of hu- man intelligence” (Bloomﬁeld, 1933, 11). The work de- scribes the linguistics of the Sanskrit language in the form of 3959 sutras, each very efﬁciently (since it had to be memorized!) expressing part of a formal rule system that brilliantly preﬁgured modern mechanisms of formal lan- guage theory (Penn and Kiparsky, 2012). One set of rules describes the k¯arakas, semantic relationships between a verb and noun arguments, roles like agent, instrument, or destination. P¯an. ini’s work was the earliest we know of that modeled the linguistic realization of events and their participants. This task of understanding how participants relate to events—being able to answer the question “Who did what to whom” (and perhaps also “when and where”)—is a central question of natural language'), Document(page_content='8.5.2 Features for CRF Named Entity Recognizers\\n\\nA CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 8.15.\\n\\nidentity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words presence of wi in a gazetteer wi contains a particular preﬁx (from all preﬁxes of length ≤ 4) wi contains a particular sufﬁx (from all sufﬁxes of length ≤ 4) word shape of wi, word shape of neighboring words short word shape of wi, short word shape of neighboring words gazetteer features Figure 8.15 Typical features for a feature-based NER system.'), Document(metadata={'source': '/tmp/tmpkxwhtz_7/tmp.pdf'}, page_content='(13.1) English: He wrote a letter to a friend\\n\\nJapanese:\\n\\ntomodachi friend\\n\\nni to\\n\\ntegami-o letter\\n\\nkaita wrote\\n\\nNote that the elements of the sentences are in very different places in the different languages. In English, the verb is in the middle of the sentence, while in Japanese, the verb kaita comes at the end. The Japanese sentence doesn’t require the pronoun he, while English does.\\n\\nSuch differences between languages can be quite complex. In the following ac- tual sentence from the United Nations, notice the many changes between the Chinese sentence (we’ve given in red a word-by-word gloss of the Chinese characters) and its English equivalent produced by human translators. (13.2) 大会/General Assembly 在/on 1982年/1982 12月/December 10日/10 通过 了/adopted 第37号/37th 决议/resolution ，核准了/approved 第二 次/second 探索/exploration 及/and 和平peaceful colorblue利用/using 外 } 层空间/outer space 会议/conference 的/of 各项/various 建议/suggestions 。'), Document(page_content='parts of speech\\n\\nDionysius Thrax of Alexandria (c. 100 B.C.), or perhaps someone else (it was a long time ago), wrote a grammatical sketch of Greek (a “techn¯e”) that summarized the linguistic knowledge of his day. This work is the source of an astonishing proportion of modern linguistic vocabulary, including the words syntax, diphthong, clitic, and analogy. Also included are a description of eight parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Although earlier scholars (including Aristotle as well as the Stoics) had their own lists of parts of speech, it was Thrax’s set of eight that became the basis for descriptions of European languages for the next 2000 years. (All the way to the Schoolhouse Rock educational television shows of our childhood, which had songs about 8 parts of speech, like the late great Bob Dorough’s Conjunction Junction.) The durability of parts of speech through two millennia speaks to their centrality in models of human language.'), Document(metadata={'source': '/tmp/tmp16hw7jef/tmp.pdf'}, page_content='In this chapter we introduce a level of representation that captures the common- ality between these sentences: there was a purchase event, the participants were XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic representations , semantic roles, express the role that arguments of a predicate take in the event, codiﬁed in databases like PropBank and FrameNet. We’ll introduce semantic role labeling, the task of assigning roles to spans in sentences, and selec- tional restrictions, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible.\\n\\n1 Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based on the Sanskrit grammar of Panini. Image from the Wellcome Collection.\\n\\n2 CHAPTER 20\\n\\nSEMANTIC ROLE LABELING\\n\\n20.1 Semantic Roles'), Document(page_content='First or last word (or embedding) of antecedent/anaphor Head word (or head embedding) of antecedent/anaphor The number, gender, animacy, person, named entity type attributes of (antecedent/anaphor) length in words of (antecedent/anaphor) Type: (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun) of an- tecedent/anaphor Features of the Antecedent Entity\\n\\nEntity shape\\n\\nP-Pr-D\\n\\nThe ‘shape’ or list of types of the mentions in the antecedent entity (cluster), i.e., sequences of (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun.\\n\\nEntity attributes\\n\\nSg-F-A-3-PER The number, gender, animacy, person, named entity type\\n\\nAnt. cluster size\\n\\n3\\n\\nattributes of the antecedent entity Number of mentions in the antecedent cluster\\n\\nFeatures of the Pair of Mentions\\n\\nSentence distance Mention distance i-within-i Cosine\\n\\n1 4 F\\n\\nThe number of sentences between antecedent and anaphor The number of mentions between antecedent and anaphor Anaphor has i-within-i relation with antecedent Cosine between antecedent and anaphor embeddings'), Document(metadata={'source': '/tmp/tmpnpvt6que/tmp.pdf'}, page_content='parsing. ACL.\\n\\nNaur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz, J. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson, B. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and M. Woodger. 1960. Report on the algorithmic language ALGOL 60. CACM, 3(5):299–314. Revised in CACM 6:1, 1-17, 1963.\\n\\nNorvig, P. 1991. Techniques for automatic memoization with applications to context-free parsing. Computational Lin- guistics, 17(1):91–98.\\n\\nPercival, W. K. 1976. On the historical source of immediate constituent analysis. In J. D. McCawley, editor, Syntax and Semantics Volume 7, Notes from the Linguistic Un- derground, pages 229–242. Academic Press.\\n\\nPollard, C. and I. A. Sag. 1994. Head-Driven Phrase Struc-\\n\\nture Grammar. University of Chicago Press.\\n\\nSalomaa, A. 1969. Probabilistic and weighted grammars.\\n\\nInformation and Control, 15:529–544.\\n\\nSekine, S. and M. Collins. 1997. The evalb software. http:\\n\\n//cs.nyu.edu/cs/projects/proteus/evalb.\\n\\n25\\n\\n26 Chapter 17\\n\\nContext-Free Grammars and Constituency Parsing')]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_ensemble_retriever([0.5, 0.5])\n",
        "test_ensemble_retriever([0.1, 0.9])\n",
        "test_ensemble_retriever([0.9, 0.1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvrbojBjvyjP"
      },
      "source": [
        "## Part 4 - Implementing Router Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "leie7mEtvyjP"
      },
      "outputs": [],
      "source": [
        "router_prompt_template = \\\n",
        "    \"\"\"\n",
        "    Your task is to categorize user quiries into one of three categories: VectorStore, SearchEngine, or None.\n",
        "    Select VectorStore for questions related to Natural Language Processing or Speech Processing.\n",
        "    Choose SearchEngine for inquiries about computer science topics not involving NLP.\n",
        "    Opt for None if the question does not pertain to NLP or Computer Science.\n",
        "    Provide only the selected category as your response. Do not include any additional information.\n",
        "    {output_instruction}\n",
        "    query: {query}\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EbAr--P2vyjQ"
      },
      "outputs": [],
      "source": [
        "router_prompt_template = dedent(router_prompt_template)\n",
        "\n",
        "router_prompt = ChatPromptTemplate.from_template(\n",
        "    template=router_prompt_template\n",
        ")\n",
        "\n",
        "router_llm = ChatTogether(\n",
        "    together_api_key=os.environ[\"TOGETHER_API_KEY\"],\n",
        "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "class QueryType(BaseModel):\n",
        "    class_name: Literal[\"None\", \"SearchEngine\", \"VectorStore\"] = Field()\n",
        "\n",
        "router_parser = PydanticOutputParser(pydantic_object=QueryType)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHETN97lvyjQ",
        "outputId": "fa5042e2-7d91-4193-ea37-9ba452ace1ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='\\nYour task is to categorize user quiries into one of three categories: VectorStore, SearchEngine, or None.\\nSelect VectorStore for questions related to Natural Language Processing or Speech Processing.\\nChoose SearchEngine for inquiries about computer science topics not involving NLP.\\nOpt for None if the question does not pertain to NLP or Computer Science.\\nProvide only the selected category as your response. Do not include any additional information.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"class_name\": {\"title\": \"Class Name\", \"enum\": [\"None\", \"SearchEngine\", \"VectorStore\"], \"type\": \"string\"}}, \"required\": [\"class_name\"]}\\n```\\nquery: What is NLI?\\n')])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for test\n",
        "router_prompt.invoke({\n",
        "    \"query\": \"What is NLI?\",\n",
        "    \"output_instruction\": router_parser.get_format_instructions()\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GMOEF1P3vyjQ"
      },
      "outputs": [],
      "source": [
        "router_chain = router_prompt | router_llm | router_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM9x5kEnX-By",
        "outputId": "112e7dc8-0200-4407-8ac9-8fcbb6614f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "related_query: class_name='VectorStore'\n",
            "unrelated_query: class_name='SearchEngine'\n",
            "super_unrelated_query: class_name='None'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def test_router_chain(query):\n",
        "    return router_chain.invoke({\n",
        "        \"query\": query,\n",
        "        \"output_instruction\": router_parser.get_format_instructions()\n",
        "    })\n",
        "\n",
        "print(\n",
        "    f\"related_query: {test_router_chain('What are the main uses of Named Entity Recognizers?')}\\n\"\n",
        "    f\"unrelated_query: {test_router_chain('Who are the top providers of cloud computing services?')}\\n\"\n",
        "    f\"super_unrelated_query: {test_router_chain('Which teams have won the most Asian Cup titles?')}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJqCM4J7vyjQ"
      },
      "source": [
        "## Part 5 - Implementing Search Engine Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZIaxAcosvyjQ"
      },
      "outputs": [],
      "source": [
        "tavily_search_wrapper = TavilySearchAPIWrapper(tavily_api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "tavily_search = TavilySearchResults(api_wrapper=tavily_search_wrapper, max_results=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn0Je23aehAG",
        "outputId": "e824ac51-aac6-463b-d99f-396ebf9759c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'url': 'https://www.sciencedirect.com/science/article/pii/S2949719124000074',\n",
              "  'content': 'Natural Language Processing Journal\\nRecent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review\\nAbstract\\nSentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.\\n Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative.'},\n",
              " {'url': 'https://link.springer.com/article/10.1007/s41060-024-00594-x',\n",
              "  'content': 'In recent years, sentiment analysis has emerged as a pivotal field within natural language processing (NLP), driven by the exponential growth of digital data and the increasing need to extract insights from textual information. With the prevalence of social media platforms, online reviews, and customer feedback, understanding the sentiments expressed in a text has become essential for various ...'},\n",
              " {'url': 'https://www.researchgate.net/publication/378613766_Recent_advancements_and_challenges_of_NLP-based_sentiment_analysis_A_state-of-the-art_review',\n",
              "  'content': 'For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative.'},\n",
              " {'url': 'https://www.nature.com/collections/fbehficiij',\n",
              "  'content': \"Editors\\nSuparna De\\nUniversity of Surrey, UK\\nEinat Liebenthal\\nHarvard Medical School, USA\\nLaura Po\\nUniversity of Modena and Reggio Emilia, Italy\\nExplore content\\nAbout the journal\\nPublish with us\\nSearch\\nQuick links\\nImage credit: monsitj / Getty Images / iStock\\nScientific Reports (Sci Rep)\\nISSN 2045-2322 (online)\\nnature.com sitemap\\nAbout Nature Portfolio\\nDiscover content\\nPublishing policies\\nAuthor & Researcher services\\nLibraries & institutions\\nAdvertising & partnerships\\nProfessional development\\nRegional websites\\n Advertisement\\nCollection\\nAdvances in Natural Language Processing\\nNatural language processing (NLP) is an interdisciplinary field spanning computational science and artificial intelligence (AI), concerned with the understanding of human language, in both written and verbal forms, by machines. This Collection is dedicated to the latest research on methodology in the vast field of NLP, which addresses and carries the potential to solve at least one of the many struggles the state-of-the-art NLP approaches face. At the same time, NLP progress is halted by the limited AI hardware infrastructure which struggles to accommodate more refined NLP models, the sparsity of good-quality NLP-training data, and complex linguistic problems, such as machine's understanding of homonymy or generation of polysemy.\\n In the growing NLP domain, two main methodological branches can be distinguished: natural language understanding (NLU), which aims to improve the machine's reading comprehension, and natural language generation (NLG), focused on enabling machines to produce human language text responses based on a given data input.\\n\"},\n",
              " {'url': 'https://www.startus-insights.com/innovators-guide/natural-language-processing-trends/',\n",
              "  'content': 'Top 9 Natural Language Processing Trends\\nClick to download\\nTree Map reveals the Impact of the Top 9 Natural Language Processing Trends\\nBased on the Natural Language Processing Innovation Map, the Tree Map below illustrates the impact of the Top 9 NLP Trends in 2023. Explore our in-depth industry research on 1 645 NLP startups & scaleups and get data-driven insights into technology-based solutions in our Natural Language Processing Innovation Map!\\nNatural language processing (NLP) is a subset of AI which finds growing importance due to the increasing amount of unstructured language data. Click to download\\nSchedule Platform Demo\\nGlobal Startup Heat Map covers 1 645 Natural Language Processing Startups & Scaleups\\nThe Global Startup Heat Map below highlights the global distribution of the 1 645 exemplary startups & scaleups that we analyzed for this research. One of the barriers to effective searches is the lack of understanding of the context and intent of the input data. Innovation Map outlines the Top 9 Natural Language Processing Trends & 18 Promising Startups\\nFor this in-depth research on the Top Natural Language Processing Trends & Startups, we analyzed a sample of 1 645 global startups & scaleups.'}]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tavily_search.invoke(\"What are the latest advancements in NLP for improving sentiment analysis?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "aLS-qAotvyjQ"
      },
      "outputs": [],
      "source": [
        "@chain\n",
        "def parse_tavily_search(documents):\n",
        "    return [Document(\n",
        "              page_content=doc['content'],\n",
        "              metadata={'url': doc['url']}\n",
        "            ) for doc in documents]\n",
        "\n",
        "    return result_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gZ4qdUcnvyjR"
      },
      "outputs": [],
      "source": [
        "search_engine_chain = tavily_search | parse_tavily_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvDNKXpXeckY",
        "outputId": "00aa62a6-77b5-4452-8765-3a448ae9b954"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'url': 'https://www.sciencedirect.com/science/article/pii/S2949719124000074'}, page_content='Natural Language Processing Journal\\nRecent advancements and challenges of NLP-based sentiment analysis: A state-of-the-art review\\nAbstract\\nSentiment analysis is a method within natural language processing that evaluates and identifies the emotional tone or mood conveyed in textual data. This extensive review provides a complete understanding of sentiment analysis, covering its models, application domains, results analysis, challenges, and research directions.\\n Therefore, in this extensive survey, we began exploring the vast array of application domains for sentiment analysis, scrutinizing them within the context of existing research. The significance of sentiment analysis lies in its capacity to derive valuable insights from extensive textual data, empowering businesses to grasp customer sentiments, make informed choices, and enhance their offerings. For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative.'),\n",
              " Document(metadata={'url': 'https://link.springer.com/article/10.1007/s41060-024-00594-x'}, page_content='In recent years, sentiment analysis has emerged as a pivotal field within natural language processing (NLP), driven by the exponential growth of digital data and the increasing need to extract insights from textual information. With the prevalence of social media platforms, online reviews, and customer feedback, understanding the sentiments expressed in a text has become essential for various ...'),\n",
              " Document(metadata={'url': 'https://www.intechopen.com/chapters/87589'}, page_content='The 2023 Sentiment Analysis Roadmap\\nWritten By\\nPublished: 10 January 2024\\nDOI: 10.5772/intechopen.112276\\nCite this chapter\\nThere are two ways to cite this chapter:\\nFrom the Edited Volume\\nAdvances in Sentiment Analysis - Techniques, Applications, and Challenges\\nEdited by Jinfeng Li\\nTo purchase hard copies of this book, please contact the representative in India:\\nCBS Publishers & Distributors Pvt. Methodologies in sentiment analysis\\nDepending on targeted prediction performance (accuracy and speed) versus computational cost (connected to data complexity in various projects), sentiment analysis can be undertaken in diverse ways, i.e., rule-based approach (which requires a lexicon and weightings for the wordlist to calculate the overall polarity of the text), machine learning-based approach (which requires training with manually tagged labels in order to learn new dataset by supervised learning), and a mix (hybrid approach). It covers a wide range of topics, including but not limited to the advancements in NLP techniques, the challenges of sentiment analysis in social media, the ethical considerations of sentiment analysis, and the future directions of the field.\\n However, as sentiment analysis techniques are implemented in diverse contexts, it becomes imperative to address the ethical considerations inherent in this practice, i.e., the key ethical concerns associated with sentiment analysis as summarized below, including biases and limitations, privacy and consent, and the responsible use of sentiment analysis in sensitive domains.\\n [3] to analyze and understand the emotions, opinions, and attitudes expressed in unstructured text data (including but not limited to social media posts, customer reviews, and other online content), and hence to gauge the overall sentiment or opinion (how people feel) of a particular topic or brand (by assessing the polarity of the text) for informed decision making.'),\n",
              " Document(metadata={'url': 'https://www.researchgate.net/publication/378613766_Recent_advancements_and_challenges_of_NLP-based_sentiment_analysis_A_state-of-the-art_review'}, page_content='For the further advancement of sentiment analysis, gaining a deep understanding of its algorithms, applications, current performance, and challenges is imperative.'),\n",
              " Document(metadata={'url': 'https://www.startus-insights.com/innovators-guide/natural-language-processing-trends/'}, page_content='Top 9 Natural Language Processing Trends\\nClick to download\\nTree Map reveals the Impact of the Top 9 Natural Language Processing Trends\\nBased on the Natural Language Processing Innovation Map, the Tree Map below illustrates the impact of the Top 9 NLP Trends in 2023. Explore our in-depth industry research on 1 645 NLP startups & scaleups and get data-driven insights into technology-based solutions in our Natural Language Processing Innovation Map!\\nNatural language processing (NLP) is a subset of AI which finds growing importance due to the increasing amount of unstructured language data. Click to download\\nSchedule Platform Demo\\nGlobal Startup Heat Map covers 1 645 Natural Language Processing Startups & Scaleups\\nThe Global Startup Heat Map below highlights the global distribution of the 1 645 exemplary startups & scaleups that we analyzed for this research. One of the barriers to effective searches is the lack of understanding of the context and intent of the input data. Innovation Map outlines the Top 9 Natural Language Processing Trends & 18 Promising Startups\\nFor this in-depth research on the Top Natural Language Processing Trends & Startups, we analyzed a sample of 1 645 global startups & scaleups.')]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search_engine_chain.invoke(\"What are the latest advancements in NLP for improving sentiment analysis?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbi91kixvyjR"
      },
      "source": [
        "## Part 6 - Implementing Relevancy Check Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fDF2hFR6vyjR"
      },
      "outputs": [],
      "source": [
        "relevancy_check_template = \\\n",
        "\"\"\"You are provided with a user question and a document. If the given document is relevant to the user question and can be used to answer it, output 'Relevant', and if not, output 'Irrelevant'. Only output the words Relevant and Irrelevant in a JSON format as described in the output instructions.\n",
        "User question: {user_query}\n",
        "Document: {retrieved_document}\n",
        "Output instruction: {output_instruction}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wSCZU9lsvyjR"
      },
      "outputs": [],
      "source": [
        "relevancy_check_template = dedent(relevancy_check_template)\n",
        "relevancy_check_prompt = ChatPromptTemplate.from_template(\n",
        "    template=relevancy_check_template\n",
        ")\n",
        "\n",
        "relevancy_check_llm = ChatTogether(\n",
        "    together_api_key=os.environ[\"TOGETHER_API_KEY\"],\n",
        "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "class RelevancyType(BaseModel):\n",
        "    class_name: Literal[\"Relevant\", \"Irrelevant\"] = Field()\n",
        "\n",
        "relevancy_check_parser = PydanticOutputParser(pydantic_object=RelevancyType)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ufv8ROMrvyjS"
      },
      "outputs": [],
      "source": [
        "relevancy_check = relevancy_check_prompt | relevancy_check_llm | relevancy_check_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hjI5StdyhNAT"
      },
      "outputs": [],
      "source": [
        "def test_relevancy_check(query, document=None):\n",
        "    if document == None:\n",
        "        document = search_engine_chain.invoke(query)[0]\n",
        "\n",
        "    result = relevancy_check.invoke({\n",
        "        \"user_query\": query,\n",
        "        \"retrieved_document\": document,\n",
        "        \"output_instruction\": relevancy_check_parser.get_format_instructions()\n",
        "    })\n",
        "\n",
        "    if result.class_name == 'Irrelevant':\n",
        "        print('Irrelevant query')\n",
        "        return result\n",
        "\n",
        "    print(f'Retrieved Document: {document}')\n",
        "    return result, document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmlWQRV6wZsL",
        "outputId": "ed321362-548b-4187-f620-3dc34b1b1f21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved Document: page_content='Using the previous example, “Goth” will be recognized as a last name rather than a subculture since the identification process determined it to be a proper noun and the classification process placed it under the category of “person.”\n",
            " You must create the rules based on two types of instruction: pattern-based rules, which relate to word forms and structure, and context-based rules like “if a contraction such as Mr. or Ms. precedes a name, then that contraction is the person’s honorific title.” Pros and cons of using named entity recognition systems\n",
            "Learn more about named entity recognition with Coursera\n",
            "You can strengthen your knowledge of natural language processing with expert-level guidance on Coursera. Coursera Footer\n",
            "Popular AI Content\n",
            "Popular Programs\n",
            "Popular Skills\n",
            "Popular Career Resources\n",
            "Coursera\n",
            "Community\n",
            "More Breaking this term down into two parts can help us better understand it:\n",
            "Named Entity: A named entity is any object that can be referenced by name in text.\n",
            "' metadata={'url': 'https://www.coursera.org/articles/named-entity-recognition'}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RelevancyType(class_name='Relevant')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result, result_document = test_relevancy_check(\"What are the main uses of Named Entity Recognizers?\")\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOkTjdsxwZjs",
        "outputId": "fa6e3385-5d72-459e-edf1-b5c8a0469d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Irrelevant query\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RelevancyType(class_name='Irrelevant')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_relevancy_check(\"What are the best practices for growing tomatoes in a home garden?\", document=result_document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7fWDfyXvyjS"
      },
      "source": [
        "## Part 7 - Implementing Fallback Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "qvtYGrf_vyjS"
      },
      "outputs": [],
      "source": [
        "fallback_template = \"\"\"You are a helpful and knowledgeable teaching assistant. Your role is to supply educational information focused on Natural Language Processing (NLP) and Speech Recognition to the human user. Avoid addressing questions that fall outside the scope of NLP and Speech Recognition. If a query is not relevant, please acknowledge this limitation.\n",
        "Current conversation:\n",
        "{chat_history}\n",
        "Human: {query}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "O2DGLnPIvyjd"
      },
      "outputs": [],
      "source": [
        "@chain\n",
        "def chat_history_gather(state):\n",
        "    return {\n",
        "        \"chat_history\": gather_chat_history(state),\n",
        "        \"query\": state['query']\n",
        "    }\n",
        "\n",
        "def gather_chat_history(state):\n",
        "    return [(\n",
        "          f\"human: {msg.content}\"\n",
        "          if isinstance(msg, HumanMessage)\n",
        "          else f\"AI: {msg.content}\"\n",
        "      ) for msg in state[\"chat_history\"]]\n",
        "\n",
        "fallback_template = dedent(fallback_template)\n",
        "fallback_prompt = ChatPromptTemplate.from_template(\n",
        "    fallback_template\n",
        ")\n",
        "\n",
        "fallback_llm = ChatTogether(\n",
        "    together_api_key=os.environ[\"TOGETHER_API_KEY\"],\n",
        "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
        "    temperature=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "G-sgW2XBvyjd"
      },
      "outputs": [],
      "source": [
        "fallback_chain = chat_history_gather | fallback_prompt | fallback_llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jpMXRo6uCVBt"
      },
      "outputs": [],
      "source": [
        "class AgentSate(TypedDict):\n",
        "    \"\"\"Represents the current chat context and history with the agent.\"\"\"\n",
        "    query: str\n",
        "    generation: str\n",
        "    documents: list[Document]\n",
        "    chat_history: list[BaseMessage]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "5gb24cNUBOWt",
        "outputId": "a5b55fba-48f7-46da-a8a5-dddab0c1af66"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm happy to help! However, I must respectfully point out that growing tomatoes in a home garden falls outside the scope of Natural Language Processing (NLP) and Speech Recognition, which are the areas of expertise I'm trained to assist with.\\n\\nIf you're interested in learning more about NLP or Speech Recognition, I'd be delighted to provide information and answer any questions you may have. Alternatively, I can suggest some reputable resources or gardening experts who might be able to offer valuable advice on growing tomatoes!\\n\\nPlease feel free to rephrase or ask a new question within the realm of NLP or Speech Recognition, and I'll do my best to assist you.\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = AgentSate(\n",
        "    query=\"What are the best practices for growing tomatoes in a home garden?\",\n",
        "    generation=\"\",\n",
        "    documents=[],\n",
        "    chat_history=[]\n",
        ")\n",
        "\n",
        "fallback_chain.invoke(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il1PW1Hrvyje"
      },
      "source": [
        "## Part 8 - Implementing Generate with Context Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ex8HLbBYvyje"
      },
      "outputs": [],
      "source": [
        "generate_template = \"\"\"You are an intelligent and helpful assistant. Answer the user's query using only the information provided in the given context. If the context does not contain the necessary information, inform the user that you cannot answer based on the available context, and refrain from providing information outside of it.\n",
        "Context: {documents}\n",
        "Query: {query}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "FTxsaQicH2Ll"
      },
      "outputs": [],
      "source": [
        "generate_template = dedent(generate_template)\n",
        "generate_prompt = ChatPromptTemplate.from_template(generate_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cp6xyQ_fvyje"
      },
      "outputs": [],
      "source": [
        "generate_chain = generate_prompt | fallback_llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "CH2_vSNWIJfs",
        "outputId": "d726f528-225a-4d5f-a8f9-bfb27503877d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I apologize, but based on the provided context, I cannot answer your question about growing tomatoes in a home garden. The context only discusses named entity recognition, natural language processing, and Coursera, which is unrelated to gardening or growing tomatoes. I refrain from providing information outside of the context. If you have any questions about named entity recognition or related topics, I'd be happy to help!\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = AgentSate(\n",
        "    query=\"What are the best practices for growing tomatoes in a home garden?\",\n",
        "    generation=\"\",\n",
        "    documents=[result_document],\n",
        "    chat_history=[]\n",
        ")\n",
        "\n",
        "generate_chain.invoke(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1NgZF7bvyje"
      },
      "source": [
        "## Part 9 - Graph Preparation using LANGGRAPH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "QzdbFx4svyjf"
      },
      "outputs": [],
      "source": [
        "def router_node(state: dict):\n",
        "    try:\n",
        "        response = question_router.invoke({\"query\": query,\n",
        "                                   \"output_instructions\": question_router_parser.get_format_instructions()\n",
        "        })\n",
        "    except:\n",
        "        return 'LLMFallback'\n",
        "\n",
        "    return 'LLMFallback' if response.class_name == 'None' else response.class_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "PZmkBXVLvyjg"
      },
      "outputs": [],
      "source": [
        "def filter_documents(state: dict):\n",
        "    relevant_documents = []\n",
        "    for document in state['documents']:\n",
        "        try:\n",
        "            relevancy_result = relevancy_check.invoke({\n",
        "                'user_query': state['query'],\n",
        "                'retrieved_document': document,\n",
        "                'output_instruction': relevancy_check_parser.get_format_instructions()\n",
        "            })\n",
        "            if relevancy_result.class_name == 'Relevant':\n",
        "                relevant_documents.append(document)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    return relevant_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "PKFmjntXvyjh"
      },
      "outputs": [],
      "source": [
        "workflow = StateGraph(AgentSate)\n",
        "\n",
        "ensemble_retriever = build_ensemble_retriever([0.5, 0.5])\n",
        "workflow.add_node('vector_store', lambda state: {'documents': ensemble_retriever.invoke(input=state['query'])})\n",
        "workflow.add_node('search_engine', lambda state: {'documents': search_engine_chain.invoke(state['query'])})\n",
        "workflow.add_node('fallback', lambda state: {'generation': fallback_chain.invoke(state)})\n",
        "workflow.add_node('generate_with_context', lambda state: {'generation': generate_chain.invoke(state)})\n",
        "workflow.add_node('filter_docs', lambda state: {'documents': filter_docs_node(state)})\n",
        "\n",
        "workflow.set_conditional_entry_point(\n",
        "    router_node,\n",
        "    {\n",
        "        'VectorStore': 'vector_store',\n",
        "        'SearchEngine': 'search_engine',\n",
        "        'LLMFallback': 'fallback',\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge('vector_store', 'filter_docs')\n",
        "workflow.add_edge('search_engine', 'filter_docs')\n",
        "workflow.add_edge('fallback', END)\n",
        "workflow.add_edge('generate_with_context', END)\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    'filter_docs',\n",
        "    lambda docs: 'generate_with_context' if len(docs) != 0 else 'search_engine',\n",
        "    {\n",
        "        'search_engine': 'search_engine',\n",
        "        'generate_with_context': 'generate_with_context'\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "VYmYya5nvyjh",
        "outputId": "0e2afd6b-d7d6-41f6-a24a-360acd4fcd99"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "app = workflow.compile(debug=False)\n",
        "display(Image(app.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API,)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7PaE4juDleSe"
      },
      "outputs": [],
      "source": [
        "def send_new_message(query, state=None):\n",
        "    if state == None:\n",
        "        state = {}\n",
        "        state['chat_history'] = []\n",
        "\n",
        "    response = app.invoke({\n",
        "        'query': query,\n",
        "        'chat_history': state['chat_history']\n",
        "    })\n",
        "\n",
        "    response['chat_history'] = [HumanMessage(response['query']), AIMessage(response['generation'])]\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "eFrj38Gdvyjh",
        "outputId": "460ee2dd-8b36-40be-d00e-645fdd86fabb"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello!\n",
              "\n",
              "LSTM (Long Short-Term Memory) is a fascinating topic in the realm of Natural Language Processing (NLP). LSTM is a type of Recurrent Neural Network (RNN) architecture that's particularly well-suited for modeling sequential data, such as speech, text, or time series data.\n",
              "\n",
              "The main advantage of LSTM over traditional RNNs is its ability to learn long-term dependencies in data. This is achieved through the use of memory cells, which allow the network to store information for extended periods of time. This is particularly useful in NLP tasks, where context and relationships between words or tokens can span multiple time steps.\n",
              "\n",
              "In the context of speech recognition, LSTM is often used as a key component in acoustic models, which aim to predict phonemes or speech sounds from audio inputs. LSTM's ability to capture long-term dependencies helps to improve the accuracy of speech recognition systems.\n",
              "\n",
              "Some of the key benefits of LSTM include:\n",
              "\n",
              "1. **Improved performance**: LSTM outperforms traditional RNNs in many NLP tasks, particularly those involving long-range dependencies.\n",
              "2. **Handling vanishing gradients**: LSTM's memory cells help to mitigate the vanishing gradient problem, which occurs when gradients are backpropagated through time, causing them to shrink exponentially.\n",
              "3. **Modeling complex patterns**: LSTM is capable of modeling complex patterns and relationships in sequential data, making it a powerful tool for tasks like language modeling, machine translation, and text classification.\n",
              "\n",
              "Would you like to know more about how LSTM is used in specific NLP tasks, such as language modeling or machine translation?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = send_new_message(\"Hello Atask! I want to know about LSTM\")\n",
        "Markdown(state['generation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "-XfNp07K0PQi",
        "outputId": "2ca8d99d-f4a3-48c9-f712-034c3377ca44"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The \"forget gate\" is a crucial component of the LSTM architecture.\n",
              "\n",
              "In an LSTM network, the forget gate is one of the three gates that regulate the flow of information into and out of the memory cell. The forget gate is responsible for deciding what information to discard or \"forget\" from the previous time step.\n",
              "\n",
              "The forget gate is a sigmoid neural network layer that takes the previous hidden state and the current input as inputs. It outputs a vector of values between 0 and 1, where 1 indicates \"completely remember\" and 0 indicates \"completely forget\". The output of the forget gate is then element-wise multiplied with the previous memory cell state, effectively \"forgetting\" or removing some information from the memory cell.\n",
              "\n",
              "The forget gate plays a crucial role in learning long-term dependencies, as it allows the LSTM network to selectively retain or discard information that is no longer relevant. By forgetting some information, the network can focus on the most important aspects of the input data and reduce the impact of irrelevant information.\n",
              "\n",
              "In the context of speech recognition, the forget gate helps the LSTM network to forget some of the acoustic features that are not relevant for predicting the next phoneme, allowing the network to focus on the most important features that are relevant for accurate speech recognition.\n",
              "\n",
              "Would you like to know more about the other gates in the LSTM architecture, such as the input gate and the output gate?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = send_new_message(\"Now I want to know about forgive gate\", state)\n",
        "Markdown(state['generation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "tLNUhLoAoqkW",
        "outputId": "483c2716-d085-4d12-9228-3e56bfa2accb"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "In recent years, several new mechanisms have been proposed to enhance the functionality of gates in LSTM networks. Here are a few examples:\n",
              "\n",
              "1. **Gated Recurrent Units (GRUs)**: While not exactly a modification to the LSTM gate, GRUs are a related type of RNN that use gates in a slightly different way. GRUs have two gates: a reset gate and an update gate. The reset gate determines how much of the previous memory to forget, while the update gate determines how much new information to add. GRUs are simpler and faster to train than LSTMs but still capture long-term dependencies effectively.\n",
              "\n",
              "2. **Coupled Input and Forget Gates**: In traditional LSTMs, the input and forget gates are learned independently. However, some researchers have proposed coupling these gates, so that the forget gate's output is used as an input to the input gate. This coupling helps the network to better balance the amount of information to forget and the amount of new information to add.\n",
              "\n",
              "3. **Gate Attention Mechanisms**: This mechanism involves adding attention weights to the gates, allowing the network to focus on specific parts of the input sequence when deciding what to remember or forget. This can be particularly useful in speech recognition tasks, where certain acoustic features may be more relevant than others.\n",
              "\n",
              "4. **Learned Normalize Gate**: This mechanism involves adding a learnable normalization factor to the gates, allowing the network to adjust the scale of the gate outputs. This can help to stabilize the learning process and improve the network's ability to capture long-term dependencies.\n",
              "\n",
              "5. **Phase-LSTM**: This is a type of LSTM that uses a phase component to modulate the gates. The phase component is a learnable parameter that captures the periodic patterns in the input sequence, allowing the network to better capture long-term dependencies.\n",
              "\n",
              "6. **Gating Mechanisms with External Memory**: This involves using external memory to store information that can be used to inform the gating mechanism. For example, the network might use an external memory to store information about the context in which a word is being used, and then use this information to decide what to remember or forget.\n",
              "\n",
              "These are just a few examples of the new mechanisms that have been proposed to enhance the functionality of gates in LSTM networks. Would you like to know more about any of these mechanisms, or is there something else you'd like to explore?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = send_new_message(\"Now tell me about new mechanisms for gates in LSTM\", state)\n",
        "Markdown(state['generation'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
