{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = open(\"./data/Tarzan.txt\", encoding=\"utf8\")\n",
    "corpus = corpus_file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Pre-process your data and train your tokenizer.\n",
    "\n",
    "For this purpose I'll use the hugging face WordPiece Tokenizer. The first thing to do is to normalize the data using the `BertNormalizer` in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we'll normalize the data using `NFD`, `LowerCase`, and `StripAccent` normalizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to pre-tokenize the data. We can use the `BertPreTokenizer` to split the text based on whitespace and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed our tokenization pipeline, we'll have to train it. We have to create a `WordPieceTrainer` and use it to train out tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train([\"./data/Tarzan.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step for out tokenizer would be adding a post_processor so that it adds special tokens to the start and end of each sentence. We'll use `TemplateProcessor` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the tokenizer is ready to encode text inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Train a bi-gram model from the corpus. Also, what's data sparsity and how will you handle it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sparsity happens when there is missing data in the corpus. In the n-grams' case we might learn many different combinations of words from our corpus but still we might encounter new combinations in the test data. In this case we estimate the probability of that combination to zero, which is not true. In order to solve this problem, there are many solutions such as add-1 smoothing, backoff, or interpolation. In this project we'll be using backoff method to solve the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step to create a bi-gram model is to create a method that will generate n-grams from tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_gram(text: str, n: int, tokenizer: Tokenizer) -> list[tuple[str]]:\n",
    "  \"\"\"\n",
    "  This method will first tokenize the `text` using the provided `tokenizer`.\n",
    "  After doing that it will create n-grams with respect to the given `n`.\n",
    "  \"\"\"\n",
    "\n",
    "  tokens = tokenizer.encode(text).tokens\n",
    "  result_n_grams = []\n",
    "  idx_range = range(len(tokens) - n + 1) if n > 0 else range(len(tokens) - n) \n",
    "  for i in idx_range:\n",
    "    result_n_grams = result_n_grams + [tuple(tokens[i:i+n])]\n",
    "  return result_n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can create n-grams, let's train our model. Our n-gram model is simply the probability of seeing a word after another:\n",
    "$$p(w_i|w^{i-1}_{i-k+1})=\\frac{count(w^i_{i-k+1})}{count(w^{i-1}_{i-k+1})}$$\n",
    "We'll write the function that will calculate this probability for each gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def train_n_gram(text: str, n: int, tokenizer: Tokenizer) -> dict[tuple[str], int]:\n",
    "  \"\"\"\n",
    "  This method calculate the probability of seeing the nth word after seeing\n",
    "  (n-1) words before it. To do it counts the number of times we've seen the\n",
    "  sentence with n words (`big_sentence_count`) and the number of times it's seen\n",
    "  the sentence with (n-1) words (`small_sentence_count`). the result will be =\n",
    "  `big_sentence_count` \\ `small_sentence_count`.\n",
    "  \"\"\"\n",
    "\n",
    "  big_sentences = Counter(get_n_gram(text, n, tokenizer))\n",
    "  small_sentences = Counter(get_n_gram(text, n - 1, tokenizer))\n",
    "\n",
    "  result = {}\n",
    "  for big_sentence, big_sentence_count in big_sentences.items():\n",
    "    small_sentence_count = small_sentences[big_sentence[:-1]]\n",
    "    result[big_sentence] = big_sentence_count / small_sentence_count\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Predict the following sentences with at least 10 more tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we are going to use backoff method to solve the data sparsity. Therefore We will need a method that will create n'-grams for n' from 1 to the designated n and use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_n_grams(text: str, n: int, tokenizer: Tokenizer) -> list[dict[tuple[str], int]]:\n",
    "  \"\"\"\n",
    "  This method will create n-grams for n from 1 to the designated `n`. Th result\n",
    "  will be a list of these trained n-grams where the index 0 of the list will\n",
    "  correspond to a uni-gram.\n",
    "  \"\"\"\n",
    "\n",
    "  result = [None] * n\n",
    "  for i in range(1, n + 1):\n",
    "    result[i - 1] = train_n_gram(text, i, tokenizer)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing that we need is a method to choose the next word with respect to the previous words and the trained n-gram. Note that the following implementation is not the best as it iterates over the dictionary's keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "\n",
    "def predict_next_word(previous_text: list[str], n_gram: dict[tuple[str], int]) -> str | None:\n",
    "    \"\"\"\n",
    "    This method simply searches for every combination of words in the n_gram\n",
    "    that matches the input text. After finding every matched combination, it\n",
    "    will make a random choice with the probabilities found in n_gram.\n",
    "    \"\"\"\n",
    "    matched_combs: list[tuple[str]] = []\n",
    "    combs_probabilities: list[int] = []\n",
    "    previous_text = tuple(previous_text)\n",
    "\n",
    "    for words_comb, probability in n_gram.items():\n",
    "       if previous_text == words_comb[:-1]:\n",
    "         matched_combs += [words_comb]\n",
    "         combs_probabilities += [probability]\n",
    "    \n",
    "    if not matched_combs:\n",
    "      return None\n",
    "\n",
    "    return choices(matched_combs, combs_probabilities)[0][-1] # Select the last word of the chosen n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function would be to predict the given text `n` times and backoff to lower n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(\n",
    "    init_sentence: str,\n",
    "    n_tokens: int,\n",
    "    n: int,\n",
    "    trained_n_grams: list[dict[list[str], int]],\n",
    "    tokenizer: Tokenizer) -> list[str]:\n",
    "  \"\"\"\n",
    "  This method will continue the given initial sentence until `n_tokens` using\n",
    "  the trained n-grams. it will also backoff to a lower n-gram when ever it\n",
    "  doesn't find the sequence in the initial n-gram.\n",
    "  \"\"\"\n",
    "\n",
    "  result = tokenizer.encode(init_sentence).tokens[:-1] # Tokenize and remove the end of sentence special token\n",
    "  for i in range(n_tokens):\n",
    "    next_token = None\n",
    "    current_n = n\n",
    "    while next_token is None:\n",
    "      next_token = predict_next_word(result[-(current_n - 1):], trained_n_grams[current_n - 1])\n",
    "      current_n -= 1\n",
    "    \n",
    "    result += [next_token]\n",
    "  \n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train out n-grams. In this case we'll use bi-grams as the strongest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_n_grams = train_n_grams(corpus, 2, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is ready, let's predict the sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he', 'turned', 'to', 'the', 'saracens', 'awaited', 'to', 'the', 'steaming', 'jungle', ',']\n",
      "['[CLS]', 'for', 'half', 'a', 'day', 'he', 'lolled', 'on', 'the', 'huge', 'back', 'and', 'nimmr', ',', 'had', 'promised', 'not', 'place', '.', '•', 'you', '‘']\n"
     ]
    }
   ],
   "source": [
    "init_sentence_1 = \"Knowing well the windings of the trail he\"\n",
    "print(predict_text(init_sentence_1, 10, 2, trained_n_grams, tokenizer))\n",
    "init_sentence_2 = \"For half a day he lolled on the huge back and\"\n",
    "print(predict_text(init_sentence_2, 10, 2, trained_n_grams, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Now do it with 3-grams and 5-grams!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 3-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_n_grams_3 = train_n_grams(corpus, 3, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he', 'took', 'with', 'seven', 'great', 'lions', 'watching', 'his', 'approach', 'the', 'princess']\n",
      "['[CLS]', 'for', 'half', 'a', 'day', 'he', 'lolled', 'on', 'the', 'huge', 'back', 'and', 'forth', ',', 'wagers', 'were', 'being', 'led', 'from', 'their', 'pursuer', 'even']\n"
     ]
    }
   ],
   "source": [
    "print(predict_text(init_sentence_1, 10, 3, trained_n_grams_3, tokenizer))\n",
    "print(predict_text(init_sentence_2, 10, 3, trained_n_grams_3, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 5-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_n_grams_5 = train_n_grams(corpus, 5, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he', 'took', 'short', 'cuts', ',', 'swinging', 'through', 'the', 'branches', 'of', 'the']\n",
      "['[CLS]', 'for', 'half', 'a', 'day', 'he', 'lolled', 'on', 'the', 'huge', 'back', 'and', 'essayed', 'to', 'say', '\"', 'eh', '?', '\"', 'and', 'to', 'yawn']\n"
     ]
    }
   ],
   "source": [
    "print(predict_text(init_sentence_1, 10, 5, trained_n_grams_5, tokenizer))\n",
    "print(predict_text(init_sentence_2, 10, 5, trained_n_grams_5, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5. Can you increase the `n` as much as you want in a n-gram model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No you can't. The first problem that occurs would be the increase of computation to train such a model. The second and more important problem would be the increase in data sparsity; As we are looking for larger combinations of words, the probability of seeing more larger combinations reduce dramatically leading the model to learn only a few combinations of words. For example, for the initial string of \"Knowing Well the windings\", there are less such combinations in the corpus to guess the fifth word from it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
