{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ali Ataollahi - 810199461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "  pattern = r'\\b\\w+\\b'\n",
    "  tokens = re.findall(pattern, text)\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1\n",
    "Which type of tokenizer does the code above suggest? Is it character-based, subword-based, or word-based? \n",
    "\n",
    "It is word-based tokenizer. The regular expression pattern r'\\\\b\\\\w+\\\\b' breaks down as follows:\n",
    "\n",
    "+ \\\\b matches a word boundary (the position between a word character and a non-word character)\n",
    "+ \\\\w+ matches one or more word characters (alphanumeric characters and underscore)\n",
    "+ The second \\\\b matches another word boundary\n",
    "\n",
    "So this regular expression will match sequences of word characters that are delimited by word boundaries on both sides. In other words, it will match whole words. This is different from a character-based tokenizer, which would split the text into individual characters. And it's also different from a subword-based tokenizer, which would split words into smaller meaningful subword units like prefixes, suffixes, and stems.\n",
    "\n",
    "Describe the problems of this type of tokenizer with examples.\n",
    "\n",
    "One limitation of this approach is that it struggles with words containing punctuation, resulting in tokens that might be difficult to comprehend. Take the word \"Can't,\" for instance, which could be tokenized as \"Can\" and \"'t\" instead of the more accurate \"Ca\" and \"n't.\" Word-based tokenizers struggle with misspelled words or words that are concatenated without spaces (e.g., \"cheezburger\" or \"iloveyou\").\n",
    "Additionally, this method fails to recognize the similarity between words sharing the same root. Consider the words \"Run\" and \"Running\"; the technique may not capture their linguistic connection. Another drawback of word-based tokenization is the creation of an extensive vocabulary. A fundamental cause of this challenge lies in the previously mentioned issue: the failure to grasp linguistic similarities. Consequently, the method stores variations of the same root word multiple times, contributing to a bloated vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'received', 'my', 'M', 'Sc', 'diploma', 'today', 'on', '2024', '02', '10', 'Excited', 'to', 'embark', 'on', 'this', 'new', 'journey', 'of', 'knowledge', 'and', 'discovery', 'MScGraduate', 'EducationMatters']\n"
     ]
    }
   ],
   "source": [
    "str = \"Just received my M.Sc. diploma today, on 2024/02/10! Excited to embark on this new journey of knowledge and discovery. #MScGraduate #EducationMatters.\"\n",
    "print(custom_tokenizer(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The abbreviation \"M.Sc.\" is incorrectly split into two separate tokens \"M\" and \"Sc\" instead of being treated as a single token. Because it has meaning when they come together.\n",
    "+ The date \"2024/02/10\" is split into three separate tokens \"2024\", \"02\", and \"10\" instead of being recognized as a single date entity.\n",
    "+ The hashtags \"#MScGraduate\" and \"#EducationMatters\" are split into separate tokens, losing the meaning and structure of the hashtags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'received', 'my', 'M.Sc', 'diploma', 'today', 'on', '2024/02/10', 'Excited', 'to', 'embark', 'on', 'this', 'new', 'journey', 'of', 'knowledge', 'and', 'discovery', 'MScGraduate', 'EducationMatters']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    pattern = pattern = r'\\b(?:[A-Z]\\.[A-Za-z]+|\\d{4}/\\d{2}/\\d{2}|\\d+(?:/\\d+)?|[A-Za-z]+)\\b'\n",
    "    # r'\\b(?:\\w+\\.?\\w*|\\d+(?:/\\d+)*(?:\\.\\d+)?)\\b'\n",
    "\n",
    "\n",
    "    tokens = re.findall(pattern, text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "text = \"Just received my M.Sc. diploma today, on 2024/02/10! Excited to embark on this new journey of knowledge and discovery. #MScGraduate #EducationMatters.\"\n",
    "tokens = custom_tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use another pattern and some of problems are resolved. But another problems remaining. Resolving this problem necessitates more sophisticated methods and advancements in the approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1\n",
    "\n",
    "Tokenizer used in each of BERT and GPT language models, which of the letter-based, sub-word-based or word-based types? Justify the reason for choosing this type of tokenizers in large language models.\n",
    "\n",
    "- BERT uses a sub-word-based tokenizer, specifically the WordPiece tokenizer. WordPiece is a type of subword tokenization algorithm that breaks down words into smaller, meaningful subword units (like \"un##case\" and \"##load\"). \n",
    "- The GPT language models (GPT-1, GPT-2, and GPT-3) use a byte-level Byte-Pair Encoding (BPE) tokenizer, which is also a sub-word-based tokenization approach.\n",
    "\n",
    "The main reason for using sub-word-based tokenizers (like WordPiece in BERT and Byte-Pair Encoding in GPT) in large language models like BERT and GPT is to effectively handle out-of-vocabulary (OOV) words. By breaking down words into smaller, meaningful subword units, these tokenizers can represent even rare or unseen words by combining known subword tokens. This helps mitigate the OOV issue, which can significantly impact the performance of language models that use a fixed vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2\n",
    "\n",
    "BERT and GPT language models have used two different algorithms to implement their custom tokenizers. Name these two algorithms and briefly describe their differences.\n",
    "\n",
    "#### BERT tokenizer algorithm\n",
    "Tokenizer used in BERT is based on the WordPiece algorithm, which is a type of subword tokenization algorithm. The WordPiece algorithm works by first building a vocabulary of the most frequent subword units (starting from individual characters and use $\\frac{\\text{Pair Frequency}}{\\text{First Element Frequency} \\times \\text{Second Element Frequency}}$ for select most frequent subword units) from the training data. It then iteratively combines the most frequent pairs of tokens to create new tokens, until a desired vocabulary size is reached. \n",
    "\n",
    "#### GPT tokenizer algorithm\n",
    "\n",
    "GPT employs the Byte-Pair Encoding (BPE) Method, a dynamic algorithm that commences by analyzing the distinct set of words within the given dataset. Subsequently, it constructs a vocabulary incorporating the symbols necessary to form these words. The algorithm then initiates the learning process by identifying and merging the most frequent consecutive pairs of symbols. The resulting merged subwords are systematically incorporated into the expanding vocabulary. This iterative merging process continues, gradually enlarging the size of the learned subwords and enhancing the model's linguistic capabilities. The dynamic nature of BPE ensures an adaptive and effective approach to language representation within GPT.\n",
    "\n",
    "#### Differences\n",
    "\n",
    "The WordPiece and BPE algorithms differ in their starting point, merging criteria, tokenization unit, and vocabulary size control. WordPiece begins with individual characters and builds subword units, merging the most frequent token pairs, and tokenizes text into subword units with direct control over vocabulary size. The Main difference between these two algorithms lies in their utilization of the scoring function. WordPiece takes into account the frequency of individual elements, whereas the BPE method exclusively focuses on the frequency of the pairs themselves. On the other hand, BPE starts with individual characters but merges them into larger subword units, merging the most frequent pairs of consecutive characters, and tokenizing text into subword units composed of one or more characters. Despite these distinctions, both algorithms share the common goal of efficiently handling out-of-vocabulary words by breaking them down into smaller, meaningful subword units, enabling language models to represent a vast vocabulary with a limited number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 \n",
    "\n",
    "Develop a basic implementation of these algorithms.\n",
    "\n",
    "#### WordPiece (for BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Th', 'is', 'da', 'r', 'kn', 'es', 's', 'is', 'ab', 'so', 'lu', 'te', 'ly', 'ki', 'll', 'in', 'g', '!', 'If', 'we', 'ev', 'er', 'ta', 'ke', 'th', 'is', 'tr', 'i', 'p', 'ag', 'ai', 'n', ',', 'it', 'mu', 'st', 'be', 'ab', 'ou', 't', 'th', 'e', 'ti', 'me', 'of', 'th', 'e', 'N', 'e', 'w', 'Mo', 'on']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def corpus_to_words(corpus: str) -> List[str]:\n",
    "    \"\"\"Split the corpus into words.\"\"\"\n",
    "    return re.findall(r\"\\w+|[.,!?;]\", corpus)\n",
    "\n",
    "def create_freq_map(words: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"Create a frequency map of words.\"\"\"\n",
    "    return dict(Counter(words))\n",
    "\n",
    "def create_initial_vocab(words: List[str]) -> set:\n",
    "    \"\"\"Create the initial vocabulary (alphabet).\"\"\"\n",
    "    vocab = set()\n",
    "    for word in words:\n",
    "        vocab.update(set(word))\n",
    "    return vocab\n",
    "\n",
    "def word_to_chars(word: str) -> List[str]:\n",
    "    \"\"\"Convert a word to a list of characters.\"\"\"\n",
    "    return list(word)\n",
    "\n",
    "def word_piece_pairs_score(splits: Dict[str, List[str]], word_freqs: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"Calculate the score for each pair of characters.\"\"\"\n",
    "    pair_scores = {}\n",
    "    for word, chars in splits.items():\n",
    "        for i in range(len(chars) - 1):\n",
    "            pair = ''.join(chars[i:i+2])\n",
    "            pair_scores[pair] = pair_scores.get(pair, 0) + word_freqs[word]\n",
    "    return pair_scores\n",
    "\n",
    "def merge_pair(splits: Dict[str, List[str]], pair: Tuple[str, str]) -> None:\n",
    "    \"\"\"Merge the best pair of characters.\"\"\"\n",
    "    for word, chars in splits.items():\n",
    "        new_chars = []\n",
    "        i = 0\n",
    "        while i < len(chars):\n",
    "            if ''.join(chars[i:i+2]) == pair:\n",
    "                new_chars.append(pair[0] + pair[1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_chars.append(chars[i])\n",
    "                i += 1\n",
    "        splits[word] = new_chars\n",
    "\n",
    "def train_word_piece(corpus: str, vocab_size: int) -> Tuple[List[str], Dict[Tuple[str, str], str]]:\n",
    "    \"\"\"Train the WordPiece model.\"\"\"\n",
    "    words = corpus_to_words(corpus)\n",
    "    word_freqs = create_freq_map(words)\n",
    "    vocab = create_initial_vocab(words)\n",
    "    splits = {word: word_to_chars(word) for word in word_freqs.keys()}\n",
    "    merges = {}\n",
    "\n",
    "    while len(vocab) < vocab_size:\n",
    "        scores = word_piece_pairs_score(splits, word_freqs)\n",
    "        best_pair = max(scores, key=scores.get)\n",
    "        merge_pair(splits, best_pair)\n",
    "        vocab.add(best_pair[0] + best_pair[1])\n",
    "        merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "\n",
    "    return list(vocab), merges\n",
    "\n",
    "def word_piece_largest_word(word: str, vocab: List[str]) -> List[str]:\n",
    "    \"\"\"Find the largest subwords in the vocabulary.\"\"\"\n",
    "    word_pieces = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        for j in range(len(word), i, -1):\n",
    "            subword = word[i:j]\n",
    "            if subword in vocab:\n",
    "                word_pieces.append(subword)\n",
    "                i = j\n",
    "                break\n",
    "    return word_pieces\n",
    "\n",
    "def tokenize(text: str, vocab: List[str], merges: Dict[Tuple[str, str], str]) -> List[str]:\n",
    "    \"\"\"Tokenize the text using the learned WordPiece model.\"\"\"\n",
    "    words = corpus_to_words(text)\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        word_pieces = word_piece_largest_word(word, vocab)\n",
    "        tokens.extend(word_pieces)\n",
    "    return tokens\n",
    "\n",
    "text_to_tokenize = \"This darkness is absolutely killing! If we ever take this trip again, it must be about the time of the New Moon\"\n",
    "\n",
    "file_path = \"data/All_Around_the_Moon.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_corpus = file.read()\n",
    "\n",
    "vocab, merges = train_word_piece(text_corpus, 300)\n",
    "tokens = tokenize(text_to_tokenize, vocab, merges)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE (for GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['This', 'darkness', 'is', 'absolutely', 'killing!', 'If', 'we', 'ever', 'take', 'this', 'trip', 'again,', 'it', 'must', 'be', 'about', 'the', 'time', 'of', 'the', 'New', 'Moon']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def bpe_pairs_score(splits: Dict[str, List], words_freq: Counter) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    Scoring function for BPE algorithm. The score of each pair is simply the\n",
    "    frequency of that pair in the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    pair_freq = defaultdict(int)\n",
    "\n",
    "    for word, split in splits.items():\n",
    "        for i in range(len(split) - 1):\n",
    "            pair_freq[(split[i], split[i + 1])] += words_freq[word]\n",
    "\n",
    "    return pair_freq\n",
    "\n",
    "def corpus_to_words(corpus: str) -> List[str]:\n",
    "    return corpus.split()\n",
    "\n",
    "def word_piece_largest_word(word: str, vocab: List[str]) -> List[str]:\n",
    "    return [word]\n",
    "\n",
    "def merge_pair(splits: Dict[str, List], pair: Tuple[str, str], vocab: List[str]) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Merge the most frequent pair in the splits.\n",
    "    \"\"\"\n",
    "    merged_pair = \"\".join(pair)\n",
    "    new_splits = {}\n",
    "\n",
    "    for word, split in splits.items():\n",
    "        new_split = []\n",
    "        i = 0\n",
    "        while i < len(split):\n",
    "            if i < len(split) - 1 and (split[i], split[i + 1]) == pair:\n",
    "                new_split.append(merged_pair)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_split.append(split[i])\n",
    "                i += 1\n",
    "\n",
    "        new_splits[word] = new_split\n",
    "\n",
    "    return new_splits\n",
    "\n",
    "def train_bpe(corpus: str, vocab_size: int) -> Tuple[List[str], Dict[Tuple[str, str], str]]:\n",
    "    \"\"\"\n",
    "    Train BPE on the given corpus and return the vocabulary and merges.\n",
    "    \"\"\"\n",
    "    words_freq = Counter(corpus_to_words(corpus))\n",
    "    vocab = list(words_freq.keys())\n",
    "\n",
    "    splits = {word: list(word) for word in vocab}\n",
    "\n",
    "    for _ in range(vocab_size):\n",
    "        pair_freq = bpe_pairs_score(splits, words_freq)\n",
    "        if not pair_freq:\n",
    "            break\n",
    "\n",
    "        best_pair = max(pair_freq, key=pair_freq.get)\n",
    "        vocab.append(\"\".join(best_pair))\n",
    "\n",
    "        splits = merge_pair(splits, best_pair, vocab)\n",
    "\n",
    "    merges = {(pair[0], pair[1]): merged for pair, merged in zip(pair_freq.keys(), vocab[vocab_size:])}\n",
    "\n",
    "    return vocab, merges\n",
    "\n",
    "text_to_tokenize = \"This darkness is absolutely killing! If we ever take this trip again, it must be about the time of the New Moon\"\n",
    "\n",
    "file_path = \"data/All_Around_the_Moon.txt\" \n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_corpus = file.read()\n",
    "\n",
    "vocab, merges = train_bpe(text_corpus, 300)\n",
    "tokens = tokenize(text_to_tokenize, vocab, merges)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 - Train the n-gram language model using the pre-processed text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data file, create a new tokenizer instance with the Byte-Pair Encoding (BPE) model, set up the normalizer and pre-tokenizer, define special tokens, create a BPE trainer, train the tokenizer on the data file using the BPE trainer, and set up the post-processor for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from random import choices\n",
    "\n",
    "from tokenizers import models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "\n",
    "file_path = open(\"./data/Tarzan.txt\", encoding=\"utf8\")\n",
    "\n",
    "file_content = file_path.read()\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase()]\n",
    ")\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "\n",
    "special_tokens = [\"[SEP]\", \"[MASK]\", \"[UNK]\", \"[PAD]\", \"[CLS]\"]\n",
    "\n",
    "token_trainer = trainers.BpeTrainer(special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.train([\"./data/Tarzan.txt\"], trainer=token_trainer)\n",
    "\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"[CLS]:0 $A:0 [SEP]:1\",  \n",
    "    pair=\"[CLS]:0 $A:0 [SEP]:1 $B:1 [SEP]:2\",\n",
    "    special_tokens=[(\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")), (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Bigram Preparation and Language Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def create_n_grams(text: str, n: int, tk: Tokenizer) -> list[tuple[str]]:\n",
    "    tokens = tk.encode(text).tokens\n",
    "    result_n_grams = []\n",
    "    idx_range = range(len(tokens) - n + 1) if n > 0 else range(len(tokens) - n)\n",
    "    for i in idx_range:\n",
    "        result_n_grams = result_n_grams + [tuple(tokens[i:i+n])]\n",
    "    return result_n_grams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data sparsity problem\n",
    "It refers to the problem where some n-grams in the test data may not be present in the training data, leading to zero probabilities and inability to generate those n-grams. To handle data sparsity, you can use techniques like smoothing (e.g., add-one smoothing, Good-Turing smoothing) or backoff models, which use lower-order n-grams when higher-order n-grams are not found in the training data.\n",
    "\n",
    "To address the data sparsity issue in n-gram language models, where some n-grams are rarely or never observed in the training data, leading to zero or inaccurate probabilities, several techniques can be employed: smoothing methods like Laplace smoothing, Good-Turing smoothing, and Kneser-Ney smoothing add small constants to n-gram counts to avoid zero probabilities; backoff models use lower-order n-gram probabilities when higher-order n-grams are unseen; class-based n-grams cluster words into classes and calculate probabilities over classes instead of individual words; interpolation combines probabilities from different n-gram models using weighted averages; higher-order n-grams capture longer contexts but increase model complexity; and neural language models like LSTMs and Transformers can learn distributed representations and mitigate data sparsity more effectively than traditional n-gram models, although at the cost of increased computational requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def create_n_grams(text: str, n: int, tk: Tokenizer) -> list[tuple[str]]:\n",
    "    tokens = tk.encode(text).tokens\n",
    "    result_n_grams = []\n",
    "    idx_range = range(len(tokens) - n + 1) if n > 0 else range(len(tokens) - n)\n",
    "    for i in idx_range:\n",
    "        result_n_grams = result_n_grams + [tuple(tokens[i:i+n])]\n",
    "    return result_n_grams\n",
    "\n",
    "def calculate_and_create_n_gram_probability(text: str, n: int, tk: Tokenizer) -> Dict[Tuple[str], float]:\n",
    "    tokens = tk.encode(text).tokens\n",
    "    result_n_grams = []\n",
    "    idx_range = range(len(tokens) - n + 1) if n > 0 else range(len(tokens) - n)\n",
    "    for i in idx_range:\n",
    "        result_n_grams.append(tuple(tokens[i:i+n]))\n",
    "    big_sentences = Counter(create_n_grams(text, n, tokenizer))\n",
    "    small_sentences = Counter(create_n_grams(text, n - 1, tokenizer))\n",
    "    result = {}\n",
    "    for big_sentence, big_sentence_count in big_sentences.items():\n",
    "        small_sentence_count = small_sentences[big_sentence[:-1]]\n",
    "        result[big_sentence] = big_sentence_count / small_sentence_count\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he', 'had', 'been', 'excited', 'himself', 'responsible', ',', 'riding', 'back', 'into', 'the']\n",
      "['[CLS]', 'for', 'half', 'a', 'day', 'he', 'lolled', 'on', 'the', 'huge', 'back', 'and', 'more', 'than', 'the', 'center', 'of', 'nimmr', 'in', 'his', 'only', 'protection']\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(prev_seq: list[str], n_gram: dict[tuple[str], int]) -> str | None:\n",
    "    matched_seqs: list[tuple[str]] = []\n",
    "    seq_probs: list[int] = []\n",
    "    prev_seq = tuple(prev_seq)\n",
    "    for words_seq, probability in n_gram.items():\n",
    "        if prev_seq == words_seq[:-1]:\n",
    "            matched_seqs += [words_seq]\n",
    "            seq_probs += [probability]\n",
    "    if not matched_seqs:\n",
    "        return None\n",
    "    return choices(matched_seqs, seq_probs)[0][-1] # Select the last word of the chosen n-gram\n",
    "\n",
    "def tokenize(\n",
    "    init_seq: str,\n",
    "    n: int,\n",
    "    trained_n_grams: list[dict[tuple[str], int]],\n",
    "    tk: Tokenizer) -> list[str]:\n",
    "    n_tokens = 10\n",
    "    result = tk.encode(init_seq).tokens[:-1]  # Tokenize and remove the end-of-sequence special token\n",
    "    current_n = n\n",
    "    for i in range(n_tokens):\n",
    "        next_token = None\n",
    "        while next_token is None and current_n > 0:\n",
    "            next_token = predict_next_word(result[-(current_n - 1):], trained_n_grams[current_n - 1])\n",
    "            current_n -= 1\n",
    "        if next_token is None:\n",
    "            break\n",
    "        result.append(next_token)\n",
    "        current_n = n  \n",
    "    return result\n",
    "\n",
    "def create_n_grams_set(text: str, n: int, tk: Tokenizer) -> list[dict[tuple[str], int]]:\n",
    "    result = [None] * n\n",
    "    for i in range(1, n + 1):\n",
    "        result[i - 1] = calculate_and_create_n_gram_probability(text, i, tk)\n",
    "    return result\n",
    "\n",
    "trained_n_grams_2 = create_n_grams_set(file_content, 2, tokenizer)\n",
    "\n",
    "init_seq_1 = \"Knowing well the windings of the trail he\"\n",
    "print(tokenize(init_seq_1, 2, trained_n_grams_2, tokenizer))\n",
    "\n",
    "init_seq_2 = \"For half a day he lolled on the huge back and\"\n",
    "print(tokenize(init_seq_2, 2, trained_n_grams_2, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he', 'took', 'with', 'seven', 'great', 'lions', 'watching', 'his', 'slow', 'ascent', 'toward']\n",
      "['[CLS]', 'for', 'half', 'a', 'day', 'he', 'lolled', 'on', 'the', 'huge', 'back', 'and', 'overtake', 'you', 'without', 'much', 'loss', 'of', 'time', '.', '\"', 'the']\n"
     ]
    }
   ],
   "source": [
    "trained_n_grams_3 = create_n_grams_set(file_content, 3, tokenizer)\n",
    "\n",
    "print(tokenize(init_seq_1, 3, trained_n_grams_3, tokenizer))\n",
    "print(tokenize(init_seq_2, 3, trained_n_grams_3, tokenizer))\n",
    "\n",
    "trained_n_grams_5 = create_n_grams_set(file_content, 5, tokenizer)\n",
    "\n",
    "print(tokenize(init_seq_1, 5, trained_n_grams_5, tokenizer))\n",
    "print(tokenize(init_seq_2, 5, trained_n_grams_5, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5\n",
    "\n",
    "Is it possible to increase the parameter n in the n-gram language model without limits? Why?\n",
    "\n",
    "No, it is not practical to increase the parameter 'n' without limitations. The primary challenge stems from the escalating computational demands for training such a model. Furthermore, a more significant concern arises from the heightened data sparsity. As we aspire to model larger combinations of words, the probability of encountering these extensive combinations diminishes substantially. This results in the model learning only a limited set of word combinations. For instance, when considering the initial phrase \"Exploring Uncharted Territories,\" the corpus contains fewer instances of such combinations, making it more challenging for the model to accurately predict the subsequent words in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 - Implementing a Test Function for N-Gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data\n",
    "data = pd.read_csv('data/google_play_store_apps_reviews.csv')\n",
    "\n",
    "# Step 2: Split the data\n",
    "train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build the n-gram Language Model\n",
    "def get_ngrams(text, n):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "def train_ngram(data, n):\n",
    "    positive_ngrams = []\n",
    "    negative_ngrams = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        grams = get_ngrams(row['review'], n)\n",
    "        if row['polarity'] == 1:\n",
    "            positive_ngrams.extend(grams)\n",
    "        elif row['polarity'] == 0:\n",
    "            negative_ngrams.extend(grams)\n",
    "\n",
    "    positive_freq = FreqDist(positive_ngrams)\n",
    "    negative_freq = FreqDist(negative_ngrams)\n",
    "\n",
    "    return positive_freq, negative_freq\n",
    "\n",
    "# Step 4: Train the Model\n",
    "n = 2  # Change to the desired n-gram size\n",
    "positive_freq, negative_freq = train_ngram(train_data, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: test the n-gram\n",
    "def test_ngram(data, positive_freq, negative_freq, n):\n",
    "    pred_labels = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        grams = get_ngrams(row['review'], n)\n",
    "        positive_prob = 1\n",
    "        negative_prob = 1\n",
    "\n",
    "        for gram in grams:\n",
    "            positive_prob *= positive_freq[gram] + 1\n",
    "            negative_prob *= negative_freq[gram] + 1\n",
    "\n",
    "        positive_prob = positive_prob * sum(negative_freq.values()) / sum(positive_freq.values())\n",
    "\n",
    "        if positive_prob > negative_prob:\n",
    "            pred_labels.append(1)\n",
    "        else:\n",
    "            pred_labels.append(0)\n",
    "\n",
    "    return pred_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Performance Evaluation and Reporting for N-Gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Evaluate the model on the test set\n",
    "n = 2  \n",
    "test_labels = test_ngram(test_data, positive_freq, negative_freq, n)\n",
    "\n",
    "true_labels = test_data['polarity'].tolist()\n",
    "\n",
    "accuracy = accuracy_score(true_labels, test_labels)\n",
    "precision = precision_score(true_labels, test_labels)\n",
    "recall = recall_score(true_labels, test_labels)\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has an accuracy of approximately 79.89%, indicating overall correctness. It exhibits high precision (94.74%), signifying accurate positive predictions, but a low recall (33.96%), suggesting it may miss a notable portion of actual positive instances. The balance between precision and recall depends on the task requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
