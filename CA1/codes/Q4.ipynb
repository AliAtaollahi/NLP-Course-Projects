{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "o7VelLEw17ka"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ali18\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk import ngrams\n",
        "from nltk.probability import FreqDist\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2ROsWwTCGyRC"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load the data\n",
        "data = pd.read_csv('data/google_play_store_apps_reviews.csv')\n",
        "\n",
        "# Step 2: Split the data\n",
        "train_data, test_data = train_test_split(data, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LCLxiqPwvdoc"
      },
      "outputs": [],
      "source": [
        "# Step 3: Build the n-gram Language Model\n",
        "def get_ngrams(text, n):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return list(ngrams(tokens, n))\n",
        "\n",
        "def train_ngram(data, n):\n",
        "    positive_ngrams = []\n",
        "    negative_ngrams = []\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        grams = get_ngrams(row['review'], n)\n",
        "        if row['polarity'] == 1:\n",
        "            positive_ngrams.extend(grams)\n",
        "        elif row['polarity'] == 0:\n",
        "            negative_ngrams.extend(grams)\n",
        "\n",
        "    positive_freq = FreqDist(positive_ngrams)\n",
        "    negative_freq = FreqDist(negative_ngrams)\n",
        "\n",
        "    return positive_freq, negative_freq\n",
        "\n",
        "# Step 4: Train the Model\n",
        "n = 2  # Change to the desired n-gram size\n",
        "positive_freq, negative_freq = train_ngram(train_data, n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eOWfIRYpwJjD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "# Step 5: test the n-gram\n",
        "def test_ngram(data, positive_freq, negative_freq, n):\n",
        "    pred_labels = []\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        grams = get_ngrams(row['review'], n)\n",
        "        positive_prob = 1\n",
        "        negative_prob = 1\n",
        "\n",
        "        for gram in grams:\n",
        "            positive_prob *= positive_freq[gram] + 1\n",
        "            negative_prob *= negative_freq[gram] + 1\n",
        "\n",
        "        positive_prob = positive_prob * sum(negative_freq.values()) / sum(positive_freq.values())\n",
        "\n",
        "        if positive_prob > negative_prob:\n",
        "            pred_labels.append(1)\n",
        "        else:\n",
        "            pred_labels.append(0)\n",
        "\n",
        "    return pred_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Score: 0.7988826815642458\n",
            "Precision: 0.9473684210526315\n",
            "Recall: 0.33962264150943394\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Evaluate the model on the test set\n",
        "n = 2  # Change to the desired n-gram size\n",
        "test_labels = test_ngram(test_data, positive_freq, negative_freq, n)\n",
        "\n",
        "# True labels from the test set\n",
        "true_labels = test_data['polarity'].tolist()\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_labels, test_labels)\n",
        "precision = precision_score(true_labels, test_labels)\n",
        "recall = recall_score(true_labels, test_labels)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy Score:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he']\n",
            "['[CLS]', 'for', 'half', 'a', 'day', 'he', 'lolled', 'on', 'the', 'huge', 'back', 'and']\n",
            "['[CLS]', 'knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he']\n",
            "['[CLS]', 'for', 'half', 'a', 'day', 'he', 'lolled', 'on', 'the', 'huge', 'back', 'and']\n",
            "['[CLS]', 'knowing', 'well', 'the', 'windings', 'of', 'the', 'trail', 'he']\n",
            "['[CLS]', 'for', 'half', 'a', 'day', 'he', 'lolled', 'on', 'the', 'huge', 'back', 'and']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from tokenizers import models as token_models\n",
        "from tokenizers import normalizers as token_normalizers\n",
        "from tokenizers import pre_tokenizers as token_pre_tokenizers\n",
        "from tokenizers import processors as token_processors\n",
        "from tokenizers import trainers as token_trainers\n",
        "from tokenizers import Tokenizer\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple\n",
        "from random import choices\n",
        "\n",
        "def read_file(file_path: str) -> str:\n",
        "    with open(file_path, encoding=\"utf8\") as file:\n",
        "        return file.read()\n",
        "\n",
        "def train_tokenizer(file_path: str, tokenizer: Tokenizer, special_tokens: List[str]) -> None:\n",
        "    tokenizer.normalizer = token_normalizers.Sequence(\n",
        "        [token_normalizers.NFKC(), token_normalizers.Lowercase(), token_normalizers.StripAccents()]\n",
        "    )\n",
        "    tokenizer.pre_tokenizer = token_pre_tokenizers.BertPreTokenizer()\n",
        "    token_trainer = token_trainers.BpeTrainer(special_tokens=special_tokens)\n",
        "    tokenizer.train([file_path], trainer=token_trainer)\n",
        "\n",
        "def initialize_tokenizer() -> Tokenizer:\n",
        "    tokenizer = Tokenizer(token_models.BPE(unk_token=\"[UNK]\"))\n",
        "    return tokenizer\n",
        "\n",
        "def configure_tokenizer_post_processor(tokenizer: Tokenizer, cls_id: int, sep_id: int) -> None:\n",
        "    tokenizer.post_processor = token_processors.TemplateProcessing(\n",
        "        single=\"[CLS]:0 $A:0 [SEP]:1\",\n",
        "        pair=\"[CLS]:0 $A:0 [SEP]:1 $B:1 [SEP]:2\",\n",
        "        special_tokens=[(\"[CLS]\", cls_id), (\"[SEP]\", sep_id)],\n",
        "    )\n",
        "\n",
        "def calculate_n_gram_probability(tokens: List[str], n_gram: Tuple[str], result_n_grams: Counter) -> float:\n",
        "    n_gram_count = result_n_grams[n_gram]\n",
        "    if len(n_gram) > 1:\n",
        "        prefix = tuple(n_gram[:-1])\n",
        "        prefix_count = result_n_grams[prefix] if prefix in result_n_grams else 0\n",
        "        return n_gram_count / prefix_count if prefix_count > 0 else 0\n",
        "    else:\n",
        "        return n_gram_count\n",
        "\n",
        "def generate_n_gram_probabilities(text: str, n: int, tk: Tokenizer) -> Dict[Tuple[str], float]:\n",
        "    tokens = tk.encode(text).tokens\n",
        "    result_n_grams = Counter(tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1))\n",
        "\n",
        "    return {n_gram: calculate_n_gram_probability(tokens, n_gram, result_n_grams) for n_gram in result_n_grams}\n",
        "\n",
        "def create_n_grams_set(text: str, n: int, tk: Tokenizer) -> List[Dict[Tuple[str], int]]:\n",
        "    return [generate_n_gram_probabilities(text, i, tk) for i in range(1, n + 1)]\n",
        "\n",
        "def predict_next_word(prev_seq: List[str], n_gram: Dict[Tuple[str], int]) -> str | None:\n",
        "    matched_seqs = [words_seq for words_seq in n_gram if prev_seq == words_seq[:-1]]\n",
        "    if not matched_seqs:\n",
        "        return None\n",
        "\n",
        "    return choices(matched_seqs, [n_gram[seq] for seq in matched_seqs])[0][-1]\n",
        "\n",
        "def generate_text(init_seq: str, n_tokens: int, n: int, trained_n_grams: List[Dict[Tuple[str], int]], tk: Tokenizer) -> List[str]:\n",
        "    result = tk.encode(init_seq).tokens[:-1]\n",
        "    current_n = n\n",
        "\n",
        "    for _ in range(n_tokens):\n",
        "        next_token = None\n",
        "\n",
        "        while next_token is None and current_n > 0:\n",
        "            next_token = predict_next_word(result[-(current_n - 1):], trained_n_grams[current_n - 1])\n",
        "            current_n -= 1\n",
        "\n",
        "        if next_token is None:\n",
        "            break\n",
        "\n",
        "        result.append(next_token)\n",
        "        current_n = n\n",
        "\n",
        "    return result\n",
        "\n",
        "file_path = \"./data/Tarzan.txt\"\n",
        "file_content = read_file(file_path)\n",
        "\n",
        "tokenizer = initialize_tokenizer()\n",
        "train_tokenizer(file_path, tokenizer, [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "\n",
        "cls_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "\n",
        "configure_tokenizer_post_processor(tokenizer, cls_id, sep_id)\n",
        "\n",
        "trained_n_grams_2 = create_n_grams_set(file_content, 2, tokenizer)\n",
        "\n",
        "init_seq_1 = \"Knowing well the windings of the trail he\"\n",
        "print(generate_text(init_seq_1, 10, 2, trained_n_grams_2, tokenizer))\n",
        "init_seq_2 = \"For half a day he lolled on the huge back and\"\n",
        "print(generate_text(init_seq_2, 10, 2, trained_n_grams_2, tokenizer))\n",
        "\n",
        "trained_n_grams_3 = create_n_grams_set(file_content, 3, tokenizer)\n",
        "\n",
        "print(generate_text(init_seq_1, 10, 3, trained_n_grams_3, tokenizer))\n",
        "print(generate_text(init_seq_2, 10, 3, trained_n_grams_3, tokenizer))\n",
        "\n",
        "trained_n_grams_5 = create_n_grams_set(file_content, 5, tokenizer)\n",
        "\n",
        "print(generate_text(init_seq_1, 10, 5, trained_n_grams_5, tokenizer))\n",
        "print(generate_text(init_seq_2, 10, 5, trained_n_grams_5, tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_file = open(\"./data/Tarzan.txt\", encoding=\"utf8\")\n",
        "corpus = corpus_file.read()\n",
        "\n",
        "from tokenizers import (\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")\n",
        "\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
        "\n",
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
        "\n",
        "tokenizer.train([\"./data/Tarzan.txt\"], trainer=trainer)\n",
        "\n",
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
        ")\n",
        "\n",
        "def get_n_gram(text: str, n: int, tokenizer: Tokenizer) -> list[tuple[str]]:\n",
        "  \"\"\"\n",
        "  This method will first tokenize the `text` using the provided `tokenizer`.\n",
        "  After doing that it will create n-grams with respect to the given `n`.\n",
        "  \"\"\"\n",
        "\n",
        "  tokens = tokenizer.encode(text).tokens\n",
        "  result_n_grams = []\n",
        "  idx_range = range(len(tokens) - n + 1) if n > 0 else range(len(tokens) - n) \n",
        "  for i in idx_range:\n",
        "    result_n_grams = result_n_grams + [tuple(tokens[i:i+n])]\n",
        "  return result_n_grams\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def train_n_gram(text: str, n: int, tokenizer: Tokenizer) -> dict[tuple[str], int]:\n",
        "  \"\"\"\n",
        "  This method calculate the probability of seeing the nth word after seeing\n",
        "  (n-1) words before it. To do it counts the number of times we've seen the\n",
        "  sentence with n words (`big_sentence_count`) and the number of times it's seen\n",
        "  the sentence with (n-1) words (`small_sentence_count`). the result will be =\n",
        "  `big_sentence_count` \\ `small_sentence_count`.\n",
        "  \"\"\"\n",
        "\n",
        "  big_sentences = Counter(get_n_gram(text, n, tokenizer))\n",
        "  small_sentences = Counter(get_n_gram(text, n - 1, tokenizer))\n",
        "\n",
        "  result = {}\n",
        "  for big_sentence, big_sentence_count in big_sentences.items():\n",
        "    small_sentence_count = small_sentences[big_sentence[:-1]]\n",
        "    result[big_sentence] = big_sentence_count / small_sentence_count\n",
        "  \n",
        "  return result\n",
        "\n",
        "\n",
        "def train_n_grams(text: str, n: int, tokenizer: Tokenizer) -> list[dict[tuple[str], int]]:\n",
        "  \"\"\"\n",
        "  This method will create n-grams for n from 1 to the designated `n`. Th result\n",
        "  will be a list of these trained n-grams where the index 0 of the list will\n",
        "  correspond to a uni-gram.\n",
        "  \"\"\"\n",
        "\n",
        "  result = [None] * n\n",
        "  for i in range(1, n + 1):\n",
        "    result[i - 1] = train_n_gram(text, i, tokenizer)\n",
        "  return result\n",
        "\n",
        "\n",
        "from random import choices\n",
        "\n",
        "def predict_next_word(previous_text: list[str], n_gram: dict[tuple[str], int]) -> str | None:\n",
        "    \"\"\"\n",
        "    This method simply searches for every combination of words in the n_gram\n",
        "    that matches the input text. After finding every matched combination, it\n",
        "    will make a random choice with the probabilities found in n_gram.\n",
        "    \"\"\"\n",
        "    matched_combs: list[tuple[str]] = []\n",
        "    combs_probabilities: list[int] = []\n",
        "    previous_text = tuple(previous_text)\n",
        "\n",
        "    for words_comb, probability in n_gram.items():\n",
        "       if previous_text == words_comb[:-1]:\n",
        "         matched_combs += [words_comb]\n",
        "         combs_probabilities += [probability]\n",
        "    \n",
        "    if not matched_combs:\n",
        "      return None\n",
        "\n",
        "    return choices(matched_combs, combs_probabilities)[0][-1] # Select the last word of the chosen n-gram\n",
        "\n",
        "\n",
        "def predict_text(\n",
        "    init_sentence: str,\n",
        "    n_tokens: int,\n",
        "    n: int,\n",
        "    trained_n_grams: list[dict[list[str], int]],\n",
        "    tokenizer: Tokenizer) -> list[str]:\n",
        "  \"\"\"\n",
        "  This method will continue the given initial sentence until `n_tokens` using\n",
        "  the trained n-grams. it will also backoff to a lower n-gram when ever it\n",
        "  doesn't find the sequence in the initial n-gram.\n",
        "  \"\"\"\n",
        "\n",
        "  result = tokenizer.encode(init_sentence).tokens[:-1] # Tokenize and remove the end of sentence special token\n",
        "  for i in range(n_tokens):\n",
        "    next_token = None\n",
        "    current_n = n\n",
        "    while next_token is None:\n",
        "      next_token = predict_next_word(result[-(current_n - 1):], trained_n_grams[current_n - 1])\n",
        "      current_n -= 1\n",
        "    \n",
        "    result += [next_token]\n",
        "  \n",
        "  return result\n",
        "\n",
        "\n",
        "trained_n_grams = train_n_grams(corpus, 2, tokenizer)\n",
        "\n",
        "\n",
        "init_sentence_1 = \"Knowing well the windings of the trail he\"\n",
        "print(predict_text(init_sentence_1, 10, 2, trained_n_grams, tokenizer))\n",
        "init_sentence_2 = \"For half a day he lolled on the huge back and\"\n",
        "print(predict_text(init_sentence_2, 10, 2, trained_n_grams, tokenizer))\n",
        "\n",
        "\n",
        "trained_n_grams_3 = train_n_grams(corpus, 3, tokenizer)\n",
        "\n",
        "print(predict_text(init_sentence_1, 10, 3, trained_n_grams_3, tokenizer))\n",
        "print(predict_text(init_sentence_2, 10, 3, trained_n_grams_3, tokenizer))\n",
        "\n",
        "trained_n_grams_5 = train_n_grams(corpus, 5, tokenizer)\n",
        "\n",
        "print(predict_text(init_sentence_1, 10, 5, trained_n_grams_5, tokenizer))\n",
        "print(predict_text(init_sentence_2, 10, 5, trained_n_grams_5, tokenizer))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
