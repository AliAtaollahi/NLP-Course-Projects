{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ali Ataollahi - 810199461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ali18\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ali18\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_data(file_path, random_state=42, test_size=0.2, encoding='latin-1', encoding_errors='ignore', remove_stopwords=True, stemming=False):\n",
    "    data = pd.read_csv(file_path, header=None, names=['label', 'id', 'date', 'query', 'user', 'text'], encoding=encoding, encoding_errors=encoding_errors)\n",
    "\n",
    "    classes = data['label'].unique()\n",
    "\n",
    "    sampled_data = {c: data[data['label'] == c].sample(n=5000, random_state=random_state) for c in classes}\n",
    "\n",
    "    combined_data = pd.concat(sampled_data.values())\n",
    "\n",
    "    combined_data['tokens'] = combined_data['text'].apply(preprocess_text, remove_stopwords=remove_stopwords, stemming=stemming)\n",
    "\n",
    "    train_data, eval_data = train_test_split(combined_data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return train_data, eval_data\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True, stemming=False):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    if stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, eval_data = preprocess_data('data/tweets.csv', encoding='latin-1', remove_stopwords=True, stemming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>861732</th>\n",
       "      <td>4</td>\n",
       "      <td>1676735463</td>\n",
       "      <td>Fri May 01 23:00:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jzbich</td>\n",
       "      <td>@akynos - we so need to sit one day over some ...</td>\n",
       "      <td>[akynos, need, sit, one, day, drinks, convos, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790821</th>\n",
       "      <td>0</td>\n",
       "      <td>2325772421</td>\n",
       "      <td>Thu Jun 25 06:02:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Malibu_Demon</td>\n",
       "      <td>is trying to worry about too many people at on...</td>\n",
       "      <td>[trying, worry, many, people, one, time, good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81703</th>\n",
       "      <td>0</td>\n",
       "      <td>1752809650</td>\n",
       "      <td>Sat May 09 23:03:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>JasManiac</td>\n",
       "      <td>@_Raymond I was going to stream it live via my...</td>\n",
       "      <td>[going, stream, live, via, bb, storm, internet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956378</th>\n",
       "      <td>4</td>\n",
       "      <td>1825220500</td>\n",
       "      <td>Sun May 17 04:51:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>chelzmae</td>\n",
       "      <td>Had a nice time with Gokesters! Particularly K...</td>\n",
       "      <td>[nice, time, gokesters, particularly, kewl, ic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389784</th>\n",
       "      <td>4</td>\n",
       "      <td>2053159144</td>\n",
       "      <td>Sat Jun 06 03:30:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vandrefalk</td>\n",
       "      <td>thanks, so do I</td>\n",
       "      <td>[thanks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597563</th>\n",
       "      <td>4</td>\n",
       "      <td>2192959627</td>\n",
       "      <td>Tue Jun 16 07:47:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ekchan</td>\n",
       "      <td>I love the ONU Admissions BAs...always good fo...</td>\n",
       "      <td>[love, onu, admissions, bas, always, good, laugh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287472</th>\n",
       "      <td>4</td>\n",
       "      <td>2002372988</td>\n",
       "      <td>Tue Jun 02 04:01:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Caraa_x</td>\n",
       "      <td>Waiting for mum friend to come he gonna teach ...</td>\n",
       "      <td>[waiting, mum, friend, come, gon, na, teach, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885181</th>\n",
       "      <td>4</td>\n",
       "      <td>1686446949</td>\n",
       "      <td>Sun May 03 05:28:27 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Kibbely</td>\n",
       "      <td>@Lisa_Veronica hey Lisa! can't wait to see you...</td>\n",
       "      <td>[hey, lisa, ca, wait, see, us, looking, forwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119197</th>\n",
       "      <td>0</td>\n",
       "      <td>1827994269</td>\n",
       "      <td>Sun May 17 11:52:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lasharitiavuana</td>\n",
       "      <td>got back from church i broke her foot when i c...</td>\n",
       "      <td>[got, back, church, broke, foot, caught, holyg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986363</th>\n",
       "      <td>4</td>\n",
       "      <td>1834561279</td>\n",
       "      <td>Mon May 18 03:24:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ImLuked</td>\n",
       "      <td>@rafaeloyama hey, im a hey monday fan too...  ...</td>\n",
       "      <td>[rafaeloyama, hey, im, hey, monday, fan, song,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label          id                          date     query  \\\n",
       "861732       4  1676735463  Fri May 01 23:00:41 PDT 2009  NO_QUERY   \n",
       "790821       0  2325772421  Thu Jun 25 06:02:19 PDT 2009  NO_QUERY   \n",
       "81703        0  1752809650  Sat May 09 23:03:57 PDT 2009  NO_QUERY   \n",
       "956378       4  1825220500  Sun May 17 04:51:34 PDT 2009  NO_QUERY   \n",
       "1389784      4  2053159144  Sat Jun 06 03:30:26 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "1597563      4  2192959627  Tue Jun 16 07:47:57 PDT 2009  NO_QUERY   \n",
       "1287472      4  2002372988  Tue Jun 02 04:01:17 PDT 2009  NO_QUERY   \n",
       "885181       4  1686446949  Sun May 03 05:28:27 PDT 2009  NO_QUERY   \n",
       "119197       0  1827994269  Sun May 17 11:52:26 PDT 2009  NO_QUERY   \n",
       "986363       4  1834561279  Mon May 18 03:24:13 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "861732            jzbich  @akynos - we so need to sit one day over some ...   \n",
       "790821      Malibu_Demon  is trying to worry about too many people at on...   \n",
       "81703          JasManiac  @_Raymond I was going to stream it live via my...   \n",
       "956378          chelzmae  Had a nice time with Gokesters! Particularly K...   \n",
       "1389784       vandrefalk                                   thanks, so do I    \n",
       "...                  ...                                                ...   \n",
       "1597563           ekchan  I love the ONU Admissions BAs...always good fo...   \n",
       "1287472          Caraa_x  Waiting for mum friend to come he gonna teach ...   \n",
       "885181           Kibbely  @Lisa_Veronica hey Lisa! can't wait to see you...   \n",
       "119197   lasharitiavuana  got back from church i broke her foot when i c...   \n",
       "986363           ImLuked  @rafaeloyama hey, im a hey monday fan too...  ...   \n",
       "\n",
       "                                                    tokens  \n",
       "861732   [akynos, need, sit, one, day, drinks, convos, ...  \n",
       "790821   [trying, worry, many, people, one, time, good,...  \n",
       "81703    [going, stream, live, via, bb, storm, internet...  \n",
       "956378   [nice, time, gokesters, particularly, kewl, ic...  \n",
       "1389784                                           [thanks]  \n",
       "...                                                    ...  \n",
       "1597563  [love, onu, admissions, bas, always, good, laugh]  \n",
       "1287472  [waiting, mum, friend, come, gon, na, teach, d...  \n",
       "885181   [hey, lisa, ca, wait, see, us, looking, forwar...  \n",
       "119197   [got, back, church, broke, foot, caught, holyg...  \n",
       "986363   [rafaeloyama, hey, im, hey, monday, fan, song,...  \n",
       "\n",
       "[8000 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1229268</th>\n",
       "      <td>4</td>\n",
       "      <td>1991330743</td>\n",
       "      <td>Mon Jun 01 06:50:34 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BRUNETTEBARBIEE</td>\n",
       "      <td>Found the perfect title for my portfolio.  its...</td>\n",
       "      <td>[found, perfect, title, portfolio, song, love]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142590</th>\n",
       "      <td>0</td>\n",
       "      <td>1881517921</td>\n",
       "      <td>Fri May 22 04:52:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>petdoctorforum</td>\n",
       "      <td>my new anti-virus ive just installed seems to ...</td>\n",
       "      <td>[new, ive, installed, seems, effective, strugg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377131</th>\n",
       "      <td>0</td>\n",
       "      <td>2051830441</td>\n",
       "      <td>Fri Jun 05 22:55:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>juliecastle_9</td>\n",
       "      <td>fact: i still love him...you learn from mistak...</td>\n",
       "      <td>[fact, still, love, learn, mistakes, hope, rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266455</th>\n",
       "      <td>0</td>\n",
       "      <td>1988997171</td>\n",
       "      <td>Mon Jun 01 00:00:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sojufied</td>\n",
       "      <td>i think i gotta make a new twitter account cuz...</td>\n",
       "      <td>[think, got, ta, make, new, twitter, account, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454929</th>\n",
       "      <td>0</td>\n",
       "      <td>2070776908</td>\n",
       "      <td>Sun Jun 07 18:16:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>janicetertel</td>\n",
       "      <td>end of the weekend....back to work tomorrow  W...</td>\n",
       "      <td>[end, weekend, back, work, tomorrow, need, lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360949</th>\n",
       "      <td>4</td>\n",
       "      <td>2048923745</td>\n",
       "      <td>Fri Jun 05 16:00:07 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>laceygraystone</td>\n",
       "      <td>Oh AND i'm in cabin a! They're the youngest ca...</td>\n",
       "      <td>[oh, cabin, youngest, 8, going, partay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107536</th>\n",
       "      <td>4</td>\n",
       "      <td>1971592442</td>\n",
       "      <td>Sat May 30 07:27:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mynameissasha</td>\n",
       "      <td>@Blink_LapSap Take a photo with Steve tonight ...</td>\n",
       "      <td>[take, photo, steve, tonight, post, twitpic, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555722</th>\n",
       "      <td>4</td>\n",
       "      <td>2185194109</td>\n",
       "      <td>Mon Jun 15 17:01:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>kittenhotep</td>\n",
       "      <td>@lesleydenford there's this stuff called ox-e ...</td>\n",
       "      <td>[lesleydenford, stuff, called, drops, made, nz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668193</th>\n",
       "      <td>0</td>\n",
       "      <td>2245850923</td>\n",
       "      <td>Fri Jun 19 16:43:52 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>oumiec</td>\n",
       "      <td>BBL..Let the count down begin~~~~~~~~~~ 5hrs t...</td>\n",
       "      <td>[bbl, let, count, 5hrs, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837652</th>\n",
       "      <td>4</td>\n",
       "      <td>1558825582</td>\n",
       "      <td>Sun Apr 19 09:30:26 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mrlarrygreen</td>\n",
       "      <td>Good Morning and God Bless! I'm going to get m...</td>\n",
       "      <td>[good, morning, god, bless, going, get, church...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label          id                          date     query  \\\n",
       "1229268      4  1991330743  Mon Jun 01 06:50:34 PDT 2009  NO_QUERY   \n",
       "142590       0  1881517921  Fri May 22 04:52:46 PDT 2009  NO_QUERY   \n",
       "377131       0  2051830441  Fri Jun 05 22:55:06 PDT 2009  NO_QUERY   \n",
       "266455       0  1988997171  Mon Jun 01 00:00:59 PDT 2009  NO_QUERY   \n",
       "454929       0  2070776908  Sun Jun 07 18:16:45 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "1360949      4  2048923745  Fri Jun 05 16:00:07 PDT 2009  NO_QUERY   \n",
       "1107536      4  1971592442  Sat May 30 07:27:17 PDT 2009  NO_QUERY   \n",
       "1555722      4  2185194109  Mon Jun 15 17:01:25 PDT 2009  NO_QUERY   \n",
       "668193       0  2245850923  Fri Jun 19 16:43:52 PDT 2009  NO_QUERY   \n",
       "837652       4  1558825582  Sun Apr 19 09:30:26 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "1229268  BRUNETTEBARBIEE  Found the perfect title for my portfolio.  its...   \n",
       "142590    petdoctorforum  my new anti-virus ive just installed seems to ...   \n",
       "377131     juliecastle_9  fact: i still love him...you learn from mistak...   \n",
       "266455          sojufied  i think i gotta make a new twitter account cuz...   \n",
       "454929      janicetertel  end of the weekend....back to work tomorrow  W...   \n",
       "...                  ...                                                ...   \n",
       "1360949   laceygraystone  Oh AND i'm in cabin a! They're the youngest ca...   \n",
       "1107536    mynameissasha  @Blink_LapSap Take a photo with Steve tonight ...   \n",
       "1555722      kittenhotep  @lesleydenford there's this stuff called ox-e ...   \n",
       "668193            oumiec  BBL..Let the count down begin~~~~~~~~~~ 5hrs t...   \n",
       "837652      mrlarrygreen  Good Morning and God Bless! I'm going to get m...   \n",
       "\n",
       "                                                    tokens  \n",
       "1229268     [found, perfect, title, portfolio, song, love]  \n",
       "142590   [new, ive, installed, seems, effective, strugg...  \n",
       "377131   [fact, still, love, learn, mistakes, hope, rep...  \n",
       "266455   [think, got, ta, make, new, twitter, account, ...  \n",
       "454929   [end, weekend, back, work, tomorrow, need, lon...  \n",
       "...                                                    ...  \n",
       "1360949            [oh, cabin, youngest, 8, going, partay]  \n",
       "1107536  [take, photo, steve, tonight, post, twitpic, p...  \n",
       "1555722  [lesleydenford, stuff, called, drops, made, nz...  \n",
       "668193                         [bbl, let, count, 5hrs, go]  \n",
       "837652   [good, morning, god, bless, going, get, church...  \n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 - First displacement vector - TERM FREQUENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "tf_vectors_train = vectorizer.fit_transform(train_data['tokens'].apply(' '.join))\n",
    "tf_vectors_test = vectorizer.transform(eval_data['tokens'].apply(' '.join))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 - Second displacement vector - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ali18\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from hazm import Normalizer, word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "class TfIdfVectorizer:\n",
    "    def __init__(self, train_data, eval_data):\n",
    "        self.train_data = train_data\n",
    "        self.eval_data = eval_data\n",
    "        self.sentences = []\n",
    "        self.word_set = []\n",
    "        self.index_dict = {}\n",
    "        self.word_count = {}\n",
    "        self.total_documents = 0\n",
    "        self.tfidf_vectors_train = []\n",
    "        self.tfidf_vectors_test = []\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        for index, sent in self.train_data.iterrows():\n",
    "            x = [i.lower() for i in word_tokenize(sent['text']) if i.isalpha()]\n",
    "            self.sentences.append(x)\n",
    "            for word in x:\n",
    "                if word not in self.word_set:\n",
    "                    self.word_set.append(word)\n",
    "\n",
    "        self.word_set = set(self.word_set)\n",
    "        self.total_documents = len(self.sentences)\n",
    "\n",
    "    def create_index_dict(self):\n",
    "        i = 0\n",
    "        for word in self.word_set:\n",
    "            self.index_dict[word] = i\n",
    "            i += 1\n",
    "\n",
    "    def calculate_word_count(self):\n",
    "        for word in self.word_set:\n",
    "            self.word_count[word] = 0\n",
    "            for sent in self.sentences:\n",
    "                if word in sent:\n",
    "                    self.word_count[word] += 1\n",
    "\n",
    "    def termfreq(self, document, word):\n",
    "        N = len(document)\n",
    "        occurance = len([token for token in document if token == word])\n",
    "        return occurance / N\n",
    "\n",
    "    def inverse_doc_freq(self, word):\n",
    "        try:\n",
    "            word_occurance = self.word_count[word] + 1\n",
    "        except:\n",
    "            word_occurance = 1\n",
    "        return np.log(self.total_documents / word_occurance)\n",
    "\n",
    "    def tf_idf(self, sentence):\n",
    "        tf_idf_vec = np.zeros((len(self.word_set),))\n",
    "        for word in sentence:\n",
    "            tf = self.termfreq(sentence, word)\n",
    "            idf = self.inverse_doc_freq(word)\n",
    "            value = tf * idf\n",
    "            try:\n",
    "                tf_idf_vec[self.index_dict[word]] = value\n",
    "            except:\n",
    "                continue\n",
    "        return tf_idf_vec\n",
    "\n",
    "    def generate_tfidf_vectors(self):\n",
    "        for sent in self.sentences:\n",
    "            vec = self.tf_idf(sent)\n",
    "            self.tfidf_vectors_train.append(vec)\n",
    "\n",
    "        sentences_test = []\n",
    "        for index, sent in self.eval_data.iterrows():\n",
    "            x = [i.lower() for i in word_tokenize(sent['text']) if i.isalpha()]\n",
    "            sentences_test.append(x)\n",
    "\n",
    "        for sent in sentences_test:\n",
    "            vec = self.tf_idf(sent)\n",
    "            self.tfidf_vectors_test.append(vec)\n",
    "\n",
    "    def vectorize(self):\n",
    "        self.preprocess_data()\n",
    "        self.create_index_dict()\n",
    "        self.calculate_word_count()\n",
    "        self.generate_tfidf_vectors()\n",
    "        return self.tfidf_vectors_train, self.tfidf_vectors_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfIdfVectorizer(train_data, eval_data)\n",
    "tfidf_vectors_train, tfidf_vectors_test = vectorizer.vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 - Third displacement vector - PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPMIVectorizer:\n",
    "    def __init__(self, train_data, eval_data):\n",
    "        self.train_data = train_data\n",
    "        self.eval_data = eval_data\n",
    "        self.n_labels = self.train_data['label'].value_counts()\n",
    "        sentences = []\n",
    "        word_set = []\n",
    "        self.count_happy_words = {}\n",
    "        self.count_sad_words = {}\n",
    "        \n",
    "        for index, sent in train_data.iterrows():\n",
    "            x = [i.lower() for  i in word_tokenize(sent['text']) if i.isalpha()]\n",
    "            sentences.append(x)\n",
    "            for word in x:\n",
    "                if word not in word_set:\n",
    "                    word_set.append(word)\n",
    "                    \n",
    "        self.word_set = set(word_set)\n",
    "        self.index_dict = {}\n",
    "        self.create_index_dict()\n",
    "        self.sentences = sentences\n",
    "        self.word_count = self.count_dict(sentences)\n",
    "\n",
    "    # Calculates the PPMI score of a specific word in both the \"happy\" and \"sad\" label categories.\n",
    "    def calculate_PPMI(self, word):\n",
    "        # Pointwise Mutual Information (PMI) is a feature scoring metrics that estimate the association between a feature and a class.\n",
    "        n = self.train_data.shape[0]\n",
    "\n",
    "        # Calculates the probabilities of a comment belonging to each label category.\n",
    "        p_sad = self.n_labels[0] / n\n",
    "        p_happy = self.n_labels[4] / n\n",
    "\n",
    "        p_word = self.word_count[word] / n\n",
    "        p_happy_word = self.count_happy_words[word] / n\n",
    "        p_sad_word = self.count_sad_words[word] / n\n",
    "\n",
    "        happy_PMI = p_happy_word / (p_word * p_happy)\n",
    "        sad_PMI = p_sad_word / (p_word * p_sad)\n",
    "\n",
    "        happy_PPMI = 0\n",
    "        sad_PPMI = 0\n",
    "        if happy_PMI > 0:\n",
    "            happy_PPMI = happy_PMI\n",
    "        if sad_PMI > 0:\n",
    "            sad_PPMI = sad_PMI\n",
    "\n",
    "        return happy_PPMI, sad_PPMI\n",
    "\n",
    "    def create_index_dict(self):\n",
    "        i = 0\n",
    "        for word in self.word_set:\n",
    "            self.index_dict[word] = i\n",
    "            i += 1\n",
    "\n",
    "    # Calculates the PPMI vectors for a given sentence.\n",
    "    def PPMI(self, sentence):\n",
    "        PPMI_vector_sad = np.zeros((len(self.word_set),))\n",
    "        PPMI_vector_happy = np.zeros((len(self.word_set),))\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                h, s = self.calculate_PPMI(word)\n",
    "                PPMI_vector_sad[self.index_dict[word]] = s\n",
    "                PPMI_vector_happy[self.index_dict[word]] = h\n",
    "            except:\n",
    "                continue\n",
    "        return PPMI_vector_sad, PPMI_vector_happy\n",
    "    \n",
    "    def count_dict(self, sentences):\n",
    "        word_count = {}\n",
    "        for word in self.word_set:\n",
    "            word_count[word] = 0\n",
    "            for sent in sentences:\n",
    "                if word in sent:\n",
    "                    word_count[word] += 1\n",
    "        return word_count\n",
    "    \n",
    "    def vectorize(self):\n",
    "        # Tokenizing Text Data Using NLTK Library.\n",
    "        sentences_happy = []\n",
    "        sentences_sad = []\n",
    "\n",
    "        for index, sent in self.train_data.iterrows():\n",
    "            if sent['label'] == 0:\n",
    "                x_sad = [i.lower() for i in word_tokenize(sent['text']) if i.isalpha()]\n",
    "                sentences_sad.append(x_sad)\n",
    "            elif sent['label'] == 4:\n",
    "                x_happy = [i.lower() for i in word_tokenize(sent['text']) if i.isalpha()]\n",
    "                sentences_happy.append(x_happy)\n",
    "\n",
    "        self.count_happy_words = self.count_dict(sentences_happy)\n",
    "        self.count_sad_words = self.count_dict(sentences_sad)\n",
    "\n",
    "        # Generating PPMI Vectors for each comment in the training set.\n",
    "        PPMI_vector = []\n",
    "        for sent in self.sentences:\n",
    "            vec_sad, vec_happy = self.PPMI(sent)\n",
    "            PPMI_vector.append([vec_sad,vec_happy])\n",
    "\n",
    "        # Reshapes the PPMI vectors of trainig data.\n",
    "        features_train = np.array(PPMI_vector)\n",
    "        ppmi_vectors_train = features_train.reshape(features_train.shape[0],features_train.shape[1]*features_train.shape[2])\n",
    "\n",
    "        sentences_test = []\n",
    "        for index, sent in self.eval_data.iterrows():\n",
    "            x = [i.lower() for i in word_tokenize(sent['text']) if i.isalpha()]\n",
    "            sentences_test.append(x)\n",
    "\n",
    "        # Generating PPMI Vectors for each comment in the test set.\n",
    "        PPMI_vector_test = []\n",
    "        for sent in sentences_test:\n",
    "            vec_sad, vec_happy = self.PPMI(sent)\n",
    "            PPMI_vector_test.append([vec_sad,vec_happy])\n",
    "\n",
    "        # Reshapes the PPMI vectors of test data.\n",
    "        features_test = np.array(PPMI_vector_test)\n",
    "        ppmi_vectors_test = features_test.reshape(features_test.shape[0],features_test.shape[1]*features_test.shape[2])\n",
    "\n",
    "        return ppmi_vectors_train, ppmi_vectors_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_vectorizer = PPMIVectorizer(train_data, eval_data)\n",
    "ppmi_vectors_train, ppmi_vectors_test = ppmi_vectorizer.vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def apply_naive_bayes(vectorizer_train, vectorizer_test, train_labels, test_labels):\n",
    "    naive_bayes_classifier = MultinomialNB()\n",
    "    naive_bayes_classifier.fit(vectorizer_train, train_labels)\n",
    "    y_pred = naive_bayes_classifier.predict(vectorizer_test)\n",
    "    print(classification_report(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Vectors:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72      1012\n",
      "           4       0.72      0.65      0.69       988\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.71      0.70      0.70      2000\n",
      "weighted avg       0.70      0.70      0.70      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TF Vectors:\")\n",
    "apply_naive_bayes(tf_vectors_train, tf_vectors_test, train_data['label'], eval_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectors:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.72      1012\n",
      "           4       0.73      0.66      0.69       988\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.71      0.71      0.71      2000\n",
      "weighted avg       0.71      0.71      0.71      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Vectors:\")\n",
    "apply_naive_bayes(tfidf_vectors_train, tfidf_vectors_test, train_data['label'], eval_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPMI Vectors:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.78      0.73      1012\n",
      "           4       0.74      0.65      0.69       988\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.72      0.72      0.71      2000\n",
      "weighted avg       0.72      0.72      0.71      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"PPMI Vectors:\")\n",
    "apply_naive_bayes(ppmi_vectors_train, ppmi_vectors_test, train_data['label'], eval_data['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
